{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_msssim import MS_SSIM\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# TODO: make sure to .to(device) the class later, and also set up gpu\n",
    "\n",
    "# path to font list\n",
    "fonts_csv = \"fonts.csv\"\n",
    "# root directory for dataset\n",
    "dataroot = \"images\"\n",
    "# number of workers for dataloader\n",
    "workers = 0\n",
    "# number of epochs\n",
    "num_epochs = 25\n",
    "# batch size for training\n",
    "batch_size = 16\n",
    "# height and width of input image\n",
    "img_size = 64\n",
    "# the alphabet characters\n",
    "alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "# number of channels\n",
    "nc0 = 1 * len(alphabet)\n",
    "nc1 = 4 * len(alphabet)\n",
    "nc2 = 8 * len(alphabet)\n",
    "nc3 = 16 * len(alphabet)\n",
    "# disciminator channels\n",
    "dc0 = 1\n",
    "dc1 = 8\n",
    "dc2 = 16\n",
    "dc3 = 32\n",
    "# threshold\n",
    "thresh = 0\n",
    "# learning rate\n",
    "lr = 0.002\n",
    "# beta1 for Adam\n",
    "beta1 = 0.5\n",
    "# real label\n",
    "real_label = 1.0\n",
    "# fake label\n",
    "fake_label = 0.0\n",
    "# number of extra times to run the discriminator than the encdec per epoch\n",
    "num_dis = 1\n",
    "# coefficient of the discriminator loss in training\n",
    "cof_dis = 7e-3\n",
    "# number of patches to sample\n",
    "num_patches = 3\n",
    "# letter we use to generate all the other letters\n",
    "base_letter = 'R'\n",
    "# letter we are trying to generate\n",
    "gen_letter = 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FontDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.fontlist = pd.read_csv(csv_file, sep=' ')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fontlist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {}\n",
    "        for c in alphabet:\n",
    "            path = os.path.join(self.root_dir, c, f'{idx}.npy')\n",
    "            img = np.load(path)\n",
    "            img = img[img_size//2:img_size//2 + img_size, img_size//2:img_size//2 + img_size, :]\n",
    "            img = self.transform(img)\n",
    "            sample[c] = img\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://discuss.pytorch.org/t/is-there-anyway-to-do-gaussian-filtering-for-an-image-2d-3d-in-pytorch/12351/3\n",
    "def get_gaussian_kernel(kernel_size=3, sigma=2, channels=3):\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    # the product of two gaussian distributions for two different\n",
    "    # variables (in this case called x and y)\n",
    "    gaussian_kernel = (1./(2.*math.pi*variance)) *\\\n",
    "                      torch.exp(\n",
    "                          -torch.sum((xy_grid - mean)**2., dim=-1) /\\\n",
    "                          (2*variance)\n",
    "                      )\n",
    "\n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "\n",
    "    # Reshape to 2d depthwise convolutional weight\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1)\n",
    "\n",
    "    gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels, bias=False, padding=1)\n",
    "\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False\n",
    "    \n",
    "    return gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.conv01 = nn.Conv2d(dc0, dc1, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(dc1, dc2, 3, padding=1)\n",
    "        self.conv23 = nn.Conv2d(dc2, dc3, 3, padding=1)\n",
    "#         self.conv24 = nn.Conv2d(dc2, dc4, 3, padding=1)\n",
    "#         self.conv34 = nn.Conv2d(dc3, dc4, 3, padding=1)\n",
    "        \n",
    "        self.conv33 = nn.Conv2d(dc3, dc3, 3, padding=1)\n",
    "        self.conv32 = nn.Conv2d(dc3, nc0, 3, padding=1)\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(nc0, nc1, 3, padding=1)\n",
    "        self.conv0same = nn.Conv2d(nc0, nc0, 3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(nc1, nc2, 3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(nc2, nc2, 3, padding=1)\n",
    "#         self.conv2back = nn.Conv2d(nc2, nc1, 3, padding=1)\n",
    "#         self.conv1back = nn.Conv2d(nc1, nc0, 3, padding=1)\n",
    "        \n",
    "#         self.conv1strided = nn.Conv2d(nc1, nc1, 3, stride=2, padding=1)\n",
    "#         self.conv2strided = nn.Conv2d(nc2, nc2, 3, stride=2, padding=1)\n",
    "\n",
    "#         self.deconv1 = nn.ConvTranspose2d(nc1, nc0, 3, padding=1)\n",
    "#         self.deconv2 = nn.ConvTranspose2d(nc2, nc1, 3, padding=1)\n",
    "#         self.deconv1strided = nn.ConvTranspose2d(nc1, nc1, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.deconv2strided = nn.ConvTranspose2d(nc2, nc2, 3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "#         self.batchnorm0 = nn.BatchNorm2d(nc0)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(nc1)\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(nc2)\n",
    "        self.pool = nn.MaxPool2d(2, return_indices=True)\n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.threshold = nn.Threshold(thresh, 0)\n",
    "        \n",
    "        self.gaussian_filter = get_gaussian_kernel(kernel_size = 3, sigma=2, channels=nc0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv01(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv23(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x, idx1 = self.pool(x)\n",
    "        \n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x, idx2 = self.pool(x)\n",
    "        \n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x, idx3 = self.pool(x)\n",
    "        \n",
    "        x = self.unpool(x, idx3)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        x = self.unpool(x, idx2)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        x = self.unpool(x, idx1)\n",
    "        x = self.conv32(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        x = self.gaussian_filter(x)\n",
    "        x = self.conv0same(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv0same(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_EncDec(nn.Module):\n",
    "    def __init__(self, nhead=8, num_encoder_layers=6):\n",
    "        model = nn.Transformer(d_model=img_size**2, nhead=nhead, num_encoder_layers=num_encoder_layers)\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.float().masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        trans.torch.ones((1, src.shape[0], img_size*img_size))\n",
    "        for i in src.shape[0]:\n",
    "            trans[0][i] = src[i][0].reshape(-1)\n",
    "        x = model(trans, src_mask)\n",
    "        \n",
    "        return model(src, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(nc0, nc2, 3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2stri = nn.Conv2d(nc2, nc2, 4, stride=2, padding=1)\n",
    "        self.conv2same = nn.Conv2d(nc2, nc2, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(nc2, nc3, 4, stride=2, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(nc3)\n",
    "        self.conv3same = nn.Conv2d(nc3, nc3, 3, padding=1)\n",
    "        self.conv3back = nn.Conv2d(nc3, nc2, 3, padding=1)\n",
    "        self.conv0back = nn.Conv2d(nc2, nc0, 3, padding=1)\n",
    "        self.flat = nn.Flatten(2)\n",
    "        self.linear = nn.Linear(img_size//8 * img_size//8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.flat1 = nn.Flatten()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # nc0 x img_size x img_size\n",
    "        out = self.conv0(input)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc2 x img_size x img_size\n",
    "        out = self.conv2stri(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc2 x img_size/2 x img_size/2\n",
    "        out = self.conv2stri(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc2 x img_size/4 x img_size/4\n",
    "        out = self.conv2same(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2same(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc3 x img_size/8 x img_size/8\n",
    "        out = self.conv3same(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3back(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc2 x img_size/8 x img_size/8\n",
    "        out = self.conv0back(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "#         print(out.size())\n",
    "        out = self.flat(out)\n",
    "#         print(out.size())\n",
    "        out = self.linear(out)\n",
    "#         print(out.size())\n",
    "        out = self.flat1(out)\n",
    "        out = self.sigmoid(out)\n",
    "#         print(out.size())\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 1 x img_size x img_size\n",
    "            nn.Conv2d(nc0, nc2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(nc2, nc2, 4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(nc1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 4 x img_size/2 x img_size/2\n",
    "            nn.Conv2d(nc2, nc2, 4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(nc2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(nc2, nc2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nc2, nc2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 8 x img_size/4 x img_size/4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(nc2 * img_size // 4 * img_size // 4, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tensors(data):\n",
    "    tensor = None\n",
    "    \n",
    "    for c in alphabet:\n",
    "        if tensor == None:\n",
    "            tensor = data[c]\n",
    "        else:\n",
    "            tensor = torch.cat((tensor, data[c]), 1)\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset = FontDataset(csv_file=fonts_csv, \n",
    "                        root_dir=dataroot, \n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(0.5, 0.5),\n",
    "#                             AddGaussianNoise(0., 0.05),\n",
    "                        ]))\n",
    "    \n",
    "    testset_size = len(dataset) // 5 * 4\n",
    "    train_set, val_set = random_split(dataset, [testset_size, len(dataset) - testset_size], generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    train_data = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "    \n",
    "    encdec = EncoderDecoder()\n",
    "    # use this line to continue training instead of starting a new one\n",
    "    # encdec.load_state_dict(torch.load('encdec.pt'))\n",
    "    \n",
    "    criterionED_l1 = nn.L1Loss()\n",
    "    criterionED_ssim = MS_SSIM(win_size=3, data_range=1, size_average=True, channel=nc0)\n",
    "    optimizerED = optim.Adam(encdec.parameters(), lr=lr)\n",
    "\n",
    "    disc = Discriminator()\n",
    "    criterionD = nn.BCELoss()\n",
    "    optimizerD = optim.Adam(disc.parameters(), lr=lr)\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_lossD = 0.0\n",
    "        running_loss_disc = 0.0\n",
    "        running_loss_l1 = 0.0\n",
    "        running_loss_ssim = 0.0\n",
    "        \n",
    "#         for it in range(num_dis):\n",
    "#             for i, data in enumerate(train_data):\n",
    "#                 train_batch = concat_tensors(data)\n",
    "#                 disc.zero_grad()\n",
    "                \n",
    "#                 b_size = train_batch.size(0)\n",
    "#                 c_size = train_batch.size(1)\n",
    "#                 label = torch.full((b_size,c_size,), real_label, dtype=torch.float, device=device)\n",
    "                \n",
    "#                 outputD = disc(train_batch)\n",
    "#                 lossD_real = criterionD(outputD, label)\n",
    "#                 lossD_real.backward()\n",
    "\n",
    "#                 # all fake batch\n",
    "#                 outputED = encdec(train_batch)\n",
    "#                 label.fill_(fake_label)\n",
    "#                 outputD = disc(outputED.detach())\n",
    "#                 lossD_fake = criterionD(outputD, label)\n",
    "#                 lossD_fake.backward()\n",
    "\n",
    "#                 lossD = lossD_real + lossD_fake\n",
    "#                 optimizerD.step()\n",
    "#         print('finished updating dis')\n",
    "                \n",
    "        for i, data in enumerate(train_data):\n",
    "            train_batch = concat_tensors(data)\n",
    "            ###########################\n",
    "            # update disc\n",
    "            ###########################\n",
    "            disc.zero_grad()\n",
    "            # all real batch\n",
    "            b_size = train_batch.size(0)\n",
    "            c_size = train_batch.size(1)\n",
    "            label = torch.full((b_size,c_size,), real_label, dtype=torch.float, device=device)\n",
    "            outputD = disc(train_batch)\n",
    "            lossD_real = criterionD(outputD, label)\n",
    "            lossD_real.backward()\n",
    "            \n",
    "            # all fake batch\n",
    "            outputED = encdec(data[base_letter])\n",
    "            \n",
    "            label.fill_(fake_label)\n",
    "            outputD = disc(outputED.detach())\n",
    "            lossD_fake = criterionD(outputD, label)\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            lossD = lossD_real + lossD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            ###########################\n",
    "            # update encdec\n",
    "            ###########################\n",
    "            encdec.zero_grad()\n",
    "            # rerun disc\n",
    "            label.fill_(real_label)\n",
    "            outputD = disc(outputED)\n",
    "            lossED_disc = criterionD(outputD, label)\n",
    "            # run encdec\n",
    "            lossED_l1 = criterionED_l1(outputED, train_batch)\n",
    "            \n",
    "            # calculate local loss\n",
    "            lossED_local = 0.0\n",
    "            for _ in range(num_patches):\n",
    "                x = np.random.randint(16, 48)\n",
    "                y = np.random.randint(16, 48)\n",
    "                for j, c in enumerate(alphabet):\n",
    "                    outputED_patch = outputED[:, j, x-8:x+8, y-8:y+8]\n",
    "                    datac2_patch = data[c][:, 0, x-8:x+8, y-8:y+8]\n",
    "                    outputED_patch = (outputED_patch + 1) / 2\n",
    "                    datac2_patch = (datac2_patch + 1) / 2\n",
    "                    lossED_local += criterionED_l1(outputED_patch, datac2_patch)\n",
    "            \n",
    "            outputED_norm = (outputED + 1) / 2\n",
    "            train_norm = (train_batch + 1) / 2\n",
    "            lossED_ssim = 1 - criterionED_ssim(outputED_norm, train_norm)\n",
    "\n",
    "            lossED = cof_dis * lossED_disc + 0.16 * lossED_l1 + 0.84 * lossED_ssim + lossED_local/num_patches\n",
    "            lossED.backward()\n",
    "            optimizerED.step()\n",
    "    \n",
    "            running_lossD += lossD.item()\n",
    "            running_loss_disc += lossED_disc.item()\n",
    "            running_loss_l1 += lossED_l1.item()\n",
    "            running_loss_ssim += lossED_ssim.item()\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Iteration {i+1}, Loss D {running_lossD}, Loss Disc {running_loss_disc}, Loss L1 {running_loss_l1}, Loss SSIM {running_loss_ssim}\")\n",
    "                running_lossD = 0.0\n",
    "                running_loss_disc = 0.0\n",
    "                running_loss_l1 = 0.0\n",
    "                running_loss_ssim = 0.0\n",
    "                \n",
    "                fig = plt.figure(figsize=(8, 8))\n",
    "                fig.add_subplot(1, 2, 1)\n",
    "                plt.imshow(data['A'][0].permute(1, 2, 0).detach().numpy(), cmap='gray')\n",
    "                fig.add_subplot(1, 2, 2)\n",
    "                plt.imshow(outputED[0,0].detach().numpy(), cmap='gray')\n",
    "                plt.show()\n",
    "    \n",
    "#     torch.save(encdec.state_dict(), 'encdec.pt')\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Iteration 1, Loss D 1.3872934579849243, Loss Disc 0.7197707295417786, Loss L1 0.9850993752479553, Loss SSIM 0.7229686975479126\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to font list\n",
    "fonts_csv = \"fonts.csv\"\n",
    "# root directory for dataset\n",
    "dataroot = \"images\"\n",
    "# number of workers for dataloader\n",
    "workers = 0\n",
    "\n",
    "dataset = FontDataset(csv_file=fonts_csv, \n",
    "                      root_dir=dataroot, \n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize(0.5, 0.5),\n",
    "                      ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=workers)\n",
    "\n",
    "encdec = EncoderDecoder()\n",
    "encdec.load_state_dict(torch.load('encdec.pt'))\n",
    "encdec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at result and ground truth side by side\n",
    "for i, data in enumerate(dataloader):\n",
    "    if i > 50:\n",
    "        break\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    output = encdec(data['c1'])\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(data['c2'][0].permute(1, 2, 0).detach().numpy(), cmap='gray')\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(output[0].permute(1, 2, 0).detach().numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up for evaluation\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=workers)\n",
    "\n",
    "encdec = EncoderDecoder()\n",
    "encdec.load_state_dict(torch.load('encdec-Copy1.pt'))\n",
    "encdec.eval()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "criterion_ssim = MS_SSIM(win_size=3, data_range=1, size_average=True, channel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, L1: 0.11039647459983826, ssim: 0.17747068405151367\n",
      "1, L1: 0.08089693635702133, ssim: 0.09386289119720459\n",
      "2, L1: 0.06853260844945908, ssim: 0.08297383785247803\n",
      "3, L1: 0.11089372634887695, ssim: 0.23213541507720947\n",
      "4, L1: 0.038106516003608704, ssim: 0.027527153491973877\n",
      "5, L1: 0.0792616680264473, ssim: 0.11540257930755615\n",
      "6, L1: 0.04213376343250275, ssim: 0.03352046012878418\n",
      "7, L1: 0.059157781302928925, ssim: 0.07703202962875366\n",
      "8, L1: 0.24022439122200012, ssim: 0.4588257670402527\n",
      "9, L1: 0.06825708597898483, ssim: 0.07667970657348633\n",
      "10, L1: 0.051598817110061646, ssim: 0.051001548767089844\n",
      "11, L1: 0.2085886001586914, ssim: 0.2567775249481201\n",
      "12, L1: 0.07073262333869934, ssim: 0.0784960389137268\n",
      "13, L1: 0.08040574193000793, ssim: 0.09605830907821655\n",
      "14, L1: 0.05451987311244011, ssim: 0.058758437633514404\n",
      "15, L1: 0.08923070132732391, ssim: 0.15772056579589844\n",
      "16, L1: 0.11131218075752258, ssim: 0.13691407442092896\n",
      "17, L1: 0.2438725084066391, ssim: 0.4737599492073059\n",
      "18, L1: 0.08858506381511688, ssim: 0.10257601737976074\n",
      "19, L1: 0.07195201516151428, ssim: 0.0875704288482666\n",
      "20, L1: 0.09331734478473663, ssim: 0.09679895639419556\n",
      "21, L1: 0.07823194563388824, ssim: 0.08822083473205566\n",
      "22, L1: 0.11739818751811981, ssim: 0.11387735605239868\n",
      "23, L1: 0.0667552649974823, ssim: 0.06381601095199585\n",
      "24, L1: 0.11998876929283142, ssim: 0.17088258266448975\n",
      "25, L1: 0.07983861118555069, ssim: 0.08174818754196167\n",
      "26, L1: 0.0639478787779808, ssim: 0.0803343653678894\n",
      "27, L1: 0.06877158582210541, ssim: 0.07201248407363892\n",
      "28, L1: 0.057810574769973755, ssim: 0.053766727447509766\n",
      "29, L1: 0.06933445483446121, ssim: 0.06888890266418457\n",
      "30, L1: 0.07556630671024323, ssim: 0.07531929016113281\n",
      "31, L1: 0.08168505132198334, ssim: 0.12882930040359497\n",
      "32, L1: 0.07539597153663635, ssim: 0.09400874376296997\n",
      "33, L1: 0.10875514894723892, ssim: 0.14444255828857422\n",
      "34, L1: 0.10287395119667053, ssim: 0.11146926879882812\n",
      "35, L1: 0.060638319700956345, ssim: 0.06325531005859375\n",
      "36, L1: 0.06531141698360443, ssim: 0.08455002307891846\n",
      "37, L1: 0.06041226163506508, ssim: 0.059891700744628906\n",
      "38, L1: 0.11109914630651474, ssim: 0.16576504707336426\n",
      "39, L1: 0.0741693526506424, ssim: 0.08079296350479126\n",
      "40, L1: 0.08219931274652481, ssim: 0.09181523323059082\n",
      "41, L1: 0.04982729256153107, ssim: 0.07389497756958008\n",
      "42, L1: 0.07869037240743637, ssim: 0.142287015914917\n",
      "43, L1: 0.08044759184122086, ssim: 0.09641438722610474\n",
      "44, L1: 0.0699697807431221, ssim: 0.07415330410003662\n",
      "45, L1: 0.086062952876091, ssim: 0.0865662693977356\n",
      "46, L1: 0.059649527072906494, ssim: 0.06901490688323975\n",
      "47, L1: 0.05411982536315918, ssim: 0.05727654695510864\n",
      "48, L1: 0.06486041098833084, ssim: 0.058390676975250244\n",
      "49, L1: 0.06844756752252579, ssim: 0.09703141450881958\n",
      "50, L1: 0.07016517221927643, ssim: 0.07640033960342407\n",
      "51, L1: 0.07472244650125504, ssim: 0.08153796195983887\n",
      "52, L1: 0.051572803407907486, ssim: 0.05117243528366089\n",
      "53, L1: 0.07311668246984482, ssim: 0.08347994089126587\n",
      "54, L1: 0.08789613097906113, ssim: 0.10204607248306274\n",
      "55, L1: 0.08777797967195511, ssim: 0.21949148178100586\n",
      "56, L1: 0.05837231129407883, ssim: 0.06220120191574097\n",
      "57, L1: 0.09302449226379395, ssim: 0.1497597098350525\n",
      "58, L1: 0.063418447971344, ssim: 0.05179637670516968\n",
      "59, L1: 0.06547600030899048, ssim: 0.2235812544822693\n",
      "60, L1: 0.16570019721984863, ssim: 0.24871176481246948\n",
      "61, L1: 0.20832090079784393, ssim: 0.46981626749038696\n",
      "62, L1: 0.08657428622245789, ssim: 0.11344945430755615\n",
      "63, L1: 0.06240950524806976, ssim: 0.06965649127960205\n",
      "64, L1: 0.04638919234275818, ssim: 0.03624379634857178\n",
      "65, L1: 0.08564560860395432, ssim: 0.10714036226272583\n",
      "66, L1: 0.05813151225447655, ssim: 0.06423807144165039\n",
      "67, L1: 0.10618391633033752, ssim: 0.1134800910949707\n",
      "68, L1: 0.16570019721984863, ssim: 0.24871176481246948\n",
      "69, L1: 0.08349545300006866, ssim: 0.09735465049743652\n",
      "70, L1: 0.058658674359321594, ssim: 0.06384211778640747\n",
      "71, L1: 0.07996442914009094, ssim: 0.0873371958732605\n",
      "72, L1: 0.08564764261245728, ssim: 0.11042255163192749\n",
      "73, L1: 0.06957830488681793, ssim: 0.06417578458786011\n",
      "74, L1: 0.0667349100112915, ssim: 0.08756035566329956\n",
      "75, L1: 0.060447514057159424, ssim: 0.0736168622970581\n",
      "76, L1: 0.11580699682235718, ssim: 0.14616823196411133\n",
      "77, L1: 0.08010882139205933, ssim: 0.09115850925445557\n",
      "78, L1: 0.06224963814020157, ssim: 0.07315593957901001\n",
      "79, L1: 0.0801122859120369, ssim: 0.08572971820831299\n",
      "80, L1: 0.05519719049334526, ssim: 0.05046182870864868\n",
      "81, L1: 0.04991481825709343, ssim: 0.0470157265663147\n",
      "82, L1: 0.08203059434890747, ssim: 0.09390407800674438\n",
      "83, L1: 0.08293889462947845, ssim: 0.12393331527709961\n",
      "84, L1: 0.042745523154735565, ssim: 0.047422826290130615\n",
      "85, L1: 0.05421433970332146, ssim: 0.042391836643218994\n",
      "86, L1: 0.09777764976024628, ssim: 0.11692667007446289\n",
      "87, L1: 0.16442610323429108, ssim: 0.18031913042068481\n",
      "88, L1: 0.05864734202623367, ssim: 0.05882197618484497\n",
      "89, L1: 0.21817171573638916, ssim: 0.26338082551956177\n",
      "90, L1: 0.073409304022789, ssim: 0.13550305366516113\n",
      "91, L1: 0.10923173278570175, ssim: 0.14106416702270508\n",
      "92, L1: 0.09580215811729431, ssim: 0.11222904920578003\n",
      "93, L1: 0.1138966977596283, ssim: 0.14607203006744385\n",
      "94, L1: 0.06607350707054138, ssim: 0.065890371799469\n",
      "95, L1: 0.040087275207042694, ssim: 0.030010640621185303\n",
      "96, L1: 0.05828620493412018, ssim: 0.05548572540283203\n",
      "97, L1: 0.09335895627737045, ssim: 0.1558029055595398\n",
      "98, L1: 0.05157656595110893, ssim: 0.04911792278289795\n",
      "99, L1: 0.2196563482284546, ssim: 0.2701200842857361\n",
      "100, L1: 0.0709017813205719, ssim: 0.19460755586624146\n",
      "101, L1: 0.07120824605226517, ssim: 0.0683140754699707\n",
      "102, L1: 0.10556203871965408, ssim: 0.14547669887542725\n",
      "103, L1: 0.036800943315029144, ssim: 0.026219844818115234\n",
      "104, L1: 0.07244245707988739, ssim: 0.07723045349121094\n",
      "105, L1: 0.363224059343338, ssim: 0.6219459772109985\n",
      "106, L1: 0.0718672052025795, ssim: 0.06966918706893921\n",
      "107, L1: 0.06915422528982162, ssim: 0.0712231993675232\n",
      "108, L1: 0.05729978159070015, ssim: 0.05964893102645874\n",
      "109, L1: 0.08596564829349518, ssim: 0.1078413724899292\n",
      "110, L1: 0.06418215483427048, ssim: 0.07358425855636597\n",
      "111, L1: 0.05040813237428665, ssim: 0.20286911725997925\n",
      "112, L1: 0.07725847512483597, ssim: 0.08804017305374146\n",
      "113, L1: 0.05448508635163307, ssim: 0.051821112632751465\n",
      "114, L1: 0.07702110707759857, ssim: 0.08476537466049194\n",
      "115, L1: 0.03965801000595093, ssim: 0.029412388801574707\n",
      "116, L1: 0.06490347534418106, ssim: 0.07555633783340454\n",
      "117, L1: 0.08618807792663574, ssim: 0.08942347764968872\n",
      "118, L1: 0.055978137999773026, ssim: 0.057775795459747314\n",
      "119, L1: 0.07456126809120178, ssim: 0.09267371892929077\n",
      "120, L1: 0.0553063228726387, ssim: 0.05444842576980591\n",
      "121, L1: 0.08147662878036499, ssim: 0.10801106691360474\n",
      "122, L1: 0.38869813084602356, ssim: 0.6513954401016235\n",
      "123, L1: 0.09291809052228928, ssim: 0.09210264682769775\n",
      "124, L1: 0.11574260890483856, ssim: 0.2515529990196228\n",
      "125, L1: 0.1037553921341896, ssim: 0.13993853330612183\n",
      "126, L1: 0.07121125608682632, ssim: 0.07420307397842407\n",
      "127, L1: 0.06316206604242325, ssim: 0.06434404850006104\n",
      "128, L1: 0.09860745072364807, ssim: 0.11562669277191162\n",
      "129, L1: 0.0841875672340393, ssim: 0.14181244373321533\n",
      "130, L1: 0.06615540385246277, ssim: 0.07668787240982056\n",
      "131, L1: 0.0925292894244194, ssim: 0.09582686424255371\n",
      "132, L1: 0.0801507979631424, ssim: 0.07964849472045898\n",
      "133, L1: 0.33368998765945435, ssim: 0.4384918808937073\n",
      "134, L1: 0.08694475144147873, ssim: 0.10211265087127686\n",
      "135, L1: 0.06750816106796265, ssim: 0.06944459676742554\n",
      "136, L1: 0.0782369077205658, ssim: 0.09451031684875488\n",
      "137, L1: 0.11510879546403885, ssim: 0.13581758737564087\n",
      "138, L1: 0.0757659524679184, ssim: 0.09976613521575928\n",
      "139, L1: 0.04973413050174713, ssim: 0.11817502975463867\n",
      "140, L1: 0.11960969120264053, ssim: 0.19355911016464233\n",
      "141, L1: 0.05052134394645691, ssim: 0.059031009674072266\n",
      "142, L1: 0.058373790234327316, ssim: 0.06399023532867432\n",
      "143, L1: 0.06849811226129532, ssim: 0.09964889287948608\n",
      "144, L1: 0.04926455765962601, ssim: 0.04742705821990967\n",
      "145, L1: 0.23615115880966187, ssim: 0.42386651039123535\n",
      "146, L1: 0.07050785422325134, ssim: 0.07862329483032227\n",
      "147, L1: 0.07191720604896545, ssim: 0.1325216293334961\n",
      "148, L1: 0.049936674535274506, ssim: 0.046259522438049316\n",
      "149, L1: 0.07645054906606674, ssim: 0.10430055856704712\n",
      "150, L1: 0.09542341530323029, ssim: 0.09699338674545288\n",
      "151, L1: 0.09087681770324707, ssim: 0.1399936079978943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152, L1: 0.06889532506465912, ssim: 0.06919312477111816\n",
      "153, L1: 0.0607423260807991, ssim: 0.06934094429016113\n",
      "154, L1: 0.05836329609155655, ssim: 0.06114429235458374\n",
      "155, L1: 0.08883416652679443, ssim: 0.1027938723564148\n",
      "156, L1: 0.07575616985559464, ssim: 0.0858493447303772\n",
      "157, L1: 0.07978146523237228, ssim: 0.10669815540313721\n",
      "158, L1: 0.05258508026599884, ssim: 0.05345994234085083\n",
      "159, L1: 0.05873097851872444, ssim: 0.05718791484832764\n",
      "160, L1: 0.07342013716697693, ssim: 0.08319908380508423\n",
      "161, L1: 0.07586877793073654, ssim: 0.09458595514297485\n",
      "162, L1: 0.09478834271430969, ssim: 0.1022070050239563\n",
      "163, L1: 0.10470055043697357, ssim: 0.22193455696105957\n",
      "164, L1: 0.08443177491426468, ssim: 0.08526414632797241\n",
      "165, L1: 0.0715842917561531, ssim: 0.0762944221496582\n",
      "166, L1: 0.059848956763744354, ssim: 0.06284481287002563\n",
      "167, L1: 0.09003887325525284, ssim: 0.10121101140975952\n",
      "168, L1: 0.07221962511539459, ssim: 0.08247703313827515\n",
      "169, L1: 0.05456048250198364, ssim: 0.05774599313735962\n",
      "170, L1: 0.062319912016391754, ssim: 0.061666131019592285\n",
      "171, L1: 0.12235996127128601, ssim: 0.14731186628341675\n",
      "172, L1: 0.07585728913545609, ssim: 0.0846605896949768\n",
      "173, L1: 0.08897440880537033, ssim: 0.08552366495132446\n",
      "174, L1: 0.09595345705747604, ssim: 0.11795312166213989\n",
      "175, L1: 0.1530618667602539, ssim: 0.20418328046798706\n",
      "176, L1: 0.06844436377286911, ssim: 0.07597720623016357\n",
      "177, L1: 0.05680049955844879, ssim: 0.05834805965423584\n",
      "178, L1: 0.042479176074266434, ssim: 0.047439396381378174\n",
      "179, L1: 0.11444111168384552, ssim: 0.14104092121124268\n",
      "180, L1: 0.0944470688700676, ssim: 0.09846585988998413\n",
      "181, L1: 0.08431929349899292, ssim: 0.09580868482589722\n",
      "182, L1: 0.0876365378499031, ssim: 0.11429256200790405\n",
      "183, L1: 0.057242587208747864, ssim: 0.06606119871139526\n",
      "184, L1: 0.05481316149234772, ssim: 0.054457783699035645\n",
      "185, L1: 0.0776325911283493, ssim: 0.10221612453460693\n",
      "186, L1: 0.06466228514909744, ssim: 0.20862966775894165\n",
      "187, L1: 0.06182638183236122, ssim: 0.06469154357910156\n",
      "188, L1: 0.0865883082151413, ssim: 0.1480005383491516\n",
      "189, L1: 0.07957552373409271, ssim: 0.10169345140457153\n",
      "190, L1: 0.06658457964658737, ssim: 0.0711585283279419\n",
      "191, L1: 0.0835801213979721, ssim: 0.12852948904037476\n",
      "192, L1: 0.08956162631511688, ssim: 0.11507654190063477\n",
      "193, L1: 0.08366426080465317, ssim: 0.08775860071182251\n",
      "194, L1: 0.07963170111179352, ssim: 0.09295207262039185\n",
      "195, L1: 0.12010495364665985, ssim: 0.17237186431884766\n",
      "196, L1: 0.07515397667884827, ssim: 0.06803417205810547\n",
      "197, L1: 0.08823316544294357, ssim: 0.24441921710968018\n",
      "198, L1: 0.1182071790099144, ssim: 0.18135285377502441\n",
      "199, L1: 0.04738513380289078, ssim: 0.044183433055877686\n",
      "200, L1: 0.060185059905052185, ssim: 0.14968520402908325\n",
      "201, L1: 0.06966700404882431, ssim: 0.08720976114273071\n",
      "202, L1: 0.09440502524375916, ssim: 0.10351788997650146\n",
      "203, L1: 0.18186888098716736, ssim: 0.2592860460281372\n",
      "204, L1: 0.04172550514340401, ssim: 0.03743618726730347\n",
      "205, L1: 0.053161703050136566, ssim: 0.06206309795379639\n",
      "206, L1: 0.06942267715930939, ssim: 0.09048372507095337\n",
      "207, L1: 0.06128733232617378, ssim: 0.059324681758880615\n",
      "208, L1: 0.06123587489128113, ssim: 0.05832546949386597\n",
      "209, L1: 0.05999543145298958, ssim: 0.056419193744659424\n",
      "210, L1: 0.07286524772644043, ssim: 0.07574743032455444\n",
      "211, L1: 0.09657705575227737, ssim: 0.11037158966064453\n",
      "212, L1: 0.0864144042134285, ssim: 0.09380531311035156\n",
      "213, L1: 0.05756179243326187, ssim: 0.10089355707168579\n",
      "214, L1: 0.0947018563747406, ssim: 0.10982930660247803\n",
      "215, L1: 0.07384870946407318, ssim: 0.07740873098373413\n",
      "216, L1: 0.07900752127170563, ssim: 0.08479499816894531\n",
      "217, L1: 0.17347300052642822, ssim: 0.29384297132492065\n",
      "218, L1: 0.06532029807567596, ssim: 0.08351916074752808\n",
      "219, L1: 0.13060584664344788, ssim: 0.20194917917251587\n",
      "220, L1: 0.1389095038175583, ssim: 0.18415135145187378\n",
      "221, L1: 0.08162102848291397, ssim: 0.09310156106948853\n",
      "222, L1: 0.08462904393672943, ssim: 0.0932847261428833\n",
      "223, L1: 0.07755652070045471, ssim: 0.08016300201416016\n",
      "224, L1: 0.0725570023059845, ssim: 0.12490123510360718\n",
      "225, L1: 0.06201958283782005, ssim: 0.06326836347579956\n",
      "226, L1: 0.04686787724494934, ssim: 0.044676899909973145\n",
      "227, L1: 0.1690942943096161, ssim: 0.22202932834625244\n",
      "228, L1: 0.07996238023042679, ssim: 0.0848393440246582\n",
      "229, L1: 0.05495139956474304, ssim: 0.08629882335662842\n",
      "230, L1: 0.2052212804555893, ssim: 0.24015653133392334\n",
      "231, L1: 0.05728220194578171, ssim: 0.06035053730010986\n",
      "232, L1: 0.08465469628572464, ssim: 0.20122021436691284\n",
      "233, L1: 0.0871758908033371, ssim: 0.09999126195907593\n",
      "234, L1: 0.08425348252058029, ssim: 0.10168063640594482\n",
      "235, L1: 0.11844606697559357, ssim: 0.17073631286621094\n",
      "236, L1: 0.09476970881223679, ssim: 0.08210432529449463\n",
      "237, L1: 0.06849946081638336, ssim: 0.0633171796798706\n",
      "238, L1: 0.07332988828420639, ssim: 0.07410955429077148\n",
      "239, L1: 0.10579197108745575, ssim: 0.12645381689071655\n",
      "240, L1: 0.05388009920716286, ssim: 0.04750114679336548\n",
      "241, L1: 0.07468826323747635, ssim: 0.08048844337463379\n",
      "242, L1: 0.06616942584514618, ssim: 0.06644916534423828\n",
      "243, L1: 0.08787250518798828, ssim: 0.11075037717819214\n",
      "244, L1: 0.10416284203529358, ssim: 0.11556333303451538\n",
      "245, L1: 0.09403278678655624, ssim: 0.13193047046661377\n",
      "246, L1: 0.07840954512357712, ssim: 0.10303407907485962\n",
      "247, L1: 0.06790861487388611, ssim: 0.07219374179840088\n",
      "248, L1: 0.06619440019130707, ssim: 0.1070556640625\n",
      "249, L1: 0.16342364251613617, ssim: 0.21648216247558594\n",
      "250, L1: 0.05426584556698799, ssim: 0.05630171298980713\n",
      "251, L1: 0.2402423918247223, ssim: 0.32757794857025146\n",
      "252, L1: 0.07355061173439026, ssim: 0.08167952299118042\n",
      "253, L1: 0.06966373324394226, ssim: 0.07413965463638306\n",
      "254, L1: 0.05669628456234932, ssim: 0.05237644910812378\n",
      "255, L1: 0.058412693440914154, ssim: 0.051519572734832764\n",
      "256, L1: 0.097732774913311, ssim: 0.14348751306533813\n",
      "257, L1: 0.04665568843483925, ssim: 0.033098697662353516\n",
      "258, L1: 0.14095251262187958, ssim: 0.1595686674118042\n",
      "259, L1: 0.05420029163360596, ssim: 0.04765266180038452\n",
      "260, L1: 0.08288739621639252, ssim: 0.09040111303329468\n",
      "261, L1: 0.10381953418254852, ssim: 0.1591450572013855\n",
      "262, L1: 0.0835602730512619, ssim: 0.10077381134033203\n",
      "263, L1: 0.07832226157188416, ssim: 0.07500934600830078\n",
      "264, L1: 0.07392238825559616, ssim: 0.08819985389709473\n",
      "265, L1: 0.06708264350891113, ssim: 0.09616190195083618\n",
      "266, L1: 0.059720728546381, ssim: 0.062116384506225586\n",
      "267, L1: 0.09583042562007904, ssim: 0.11368948221206665\n",
      "268, L1: 0.06947579979896545, ssim: 0.07217586040496826\n",
      "269, L1: 0.06668949872255325, ssim: 0.07714807987213135\n",
      "270, L1: 0.07161005586385727, ssim: 0.08423900604248047\n",
      "271, L1: 0.07023369520902634, ssim: 0.07432836294174194\n",
      "272, L1: 0.08595754206180573, ssim: 0.09518271684646606\n",
      "273, L1: 0.10082989186048508, ssim: 0.12458795309066772\n",
      "274, L1: 0.048115409910678864, ssim: 0.03767669200897217\n",
      "275, L1: 0.0808941051363945, ssim: 0.07556438446044922\n",
      "276, L1: 0.12361115217208862, ssim: 0.11940431594848633\n",
      "277, L1: 0.06663946062326431, ssim: 0.0787515640258789\n",
      "278, L1: 0.16595958173274994, ssim: 0.2146252989768982\n",
      "279, L1: 0.06369791179895401, ssim: 0.05882817506790161\n",
      "280, L1: 0.05511406809091568, ssim: 0.05168342590332031\n",
      "281, L1: 0.08290721476078033, ssim: 0.09241241216659546\n",
      "282, L1: 0.1203908920288086, ssim: 0.22042924165725708\n",
      "283, L1: 0.1720588058233261, ssim: 0.2670275568962097\n",
      "284, L1: 0.05577537789940834, ssim: 0.06313908100128174\n",
      "285, L1: 0.07195201516151428, ssim: 0.0875704288482666\n",
      "286, L1: 0.07557158917188644, ssim: 0.0888986587524414\n",
      "287, L1: 0.06484436243772507, ssim: 0.055178046226501465\n",
      "288, L1: 0.08279614895582199, ssim: 0.07797175645828247\n",
      "289, L1: 0.11124784499406815, ssim: 0.11381286382675171\n",
      "290, L1: 0.14243760704994202, ssim: 0.17194491624832153\n",
      "291, L1: 0.10548247396945953, ssim: 0.17876434326171875\n",
      "292, L1: 0.08736469596624374, ssim: 0.09027671813964844\n",
      "293, L1: 0.12287268042564392, ssim: 0.15463876724243164\n",
      "294, L1: 0.0497937947511673, ssim: 0.050250113010406494\n",
      "295, L1: 0.059230878949165344, ssim: 0.11424052715301514\n",
      "296, L1: 0.08963367342948914, ssim: 0.12102240324020386\n",
      "297, L1: 0.1302875280380249, ssim: 0.18664777278900146\n",
      "298, L1: 0.08004625141620636, ssim: 0.07491379976272583\n",
      "299, L1: 0.07497795671224594, ssim: 0.07307237386703491\n",
      "300, L1: 0.0830988809466362, ssim: 0.08308237791061401\n",
      "301, L1: 0.09842289239168167, ssim: 0.14096713066101074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302, L1: 0.09000823646783829, ssim: 0.10436290502548218\n",
      "303, L1: 0.03278225660324097, ssim: 0.03346598148345947\n",
      "304, L1: 0.06608622521162033, ssim: 0.05992388725280762\n",
      "305, L1: 0.07300208508968353, ssim: 0.0842466950416565\n",
      "306, L1: 0.0675133690237999, ssim: 0.0772365927696228\n",
      "307, L1: 0.05704762414097786, ssim: 0.04869323968887329\n",
      "308, L1: 0.1096034049987793, ssim: 0.13312697410583496\n",
      "309, L1: 0.06016245484352112, ssim: 0.04968059062957764\n",
      "310, L1: 0.13053369522094727, ssim: 0.19141721725463867\n",
      "311, L1: 0.07232728600502014, ssim: 0.08200651407241821\n",
      "312, L1: 0.06184431537985802, ssim: 0.06264740228652954\n",
      "313, L1: 0.09022515267133713, ssim: 0.10453170537948608\n",
      "314, L1: 0.08760853856801987, ssim: 0.11121916770935059\n",
      "315, L1: 0.06120019033551216, ssim: 0.09021127223968506\n",
      "316, L1: 0.05880950391292572, ssim: 0.06488174200057983\n",
      "317, L1: 0.07695657014846802, ssim: 0.08762592077255249\n",
      "318, L1: 0.07153172791004181, ssim: 0.08104550838470459\n",
      "319, L1: 0.11937202513217926, ssim: 0.17387765645980835\n",
      "320, L1: 0.057829491794109344, ssim: 0.06199842691421509\n",
      "321, L1: 0.15086741745471954, ssim: 0.17904341220855713\n",
      "322, L1: 0.14659658074378967, ssim: 0.2676957845687866\n",
      "323, L1: 0.1499047577381134, ssim: 0.2714890241622925\n",
      "324, L1: 0.08321484923362732, ssim: 0.13656288385391235\n",
      "325, L1: 0.08886035531759262, ssim: 0.09996271133422852\n",
      "326, L1: 0.08896998316049576, ssim: 0.09781789779663086\n",
      "327, L1: 0.08234251290559769, ssim: 0.08661258220672607\n",
      "328, L1: 0.0607423260807991, ssim: 0.06934094429016113\n",
      "329, L1: 0.07111906260251999, ssim: 0.07233577966690063\n",
      "330, L1: 0.07067513465881348, ssim: 0.08010554313659668\n",
      "331, L1: 0.10411463677883148, ssim: 0.14877676963806152\n",
      "332, L1: 0.07902353256940842, ssim: 0.1579485535621643\n",
      "333, L1: 0.07486649602651596, ssim: 0.07890260219573975\n",
      "334, L1: 0.07320697605609894, ssim: 0.08896887302398682\n",
      "335, L1: 0.07104483991861343, ssim: 0.08805191516876221\n",
      "336, L1: 0.08816683292388916, ssim: 0.09256893396377563\n",
      "337, L1: 0.0623844712972641, ssim: 0.07120418548583984\n",
      "338, L1: 0.05893055349588394, ssim: 0.052982449531555176\n",
      "339, L1: 0.10152829438447952, ssim: 0.26605910062789917\n",
      "340, L1: 0.05734146386384964, ssim: 0.06710284948348999\n",
      "341, L1: 0.08044411242008209, ssim: 0.13162928819656372\n",
      "342, L1: 0.10540878027677536, ssim: 0.12518125772476196\n",
      "343, L1: 0.08080033957958221, ssim: 0.115223228931427\n",
      "344, L1: 0.07318978011608124, ssim: 0.08248287439346313\n",
      "345, L1: 0.08303725719451904, ssim: 0.09802478551864624\n",
      "346, L1: 0.05170012265443802, ssim: 0.04547780752182007\n",
      "347, L1: 0.2575848698616028, ssim: 0.35317373275756836\n",
      "348, L1: 0.11975828558206558, ssim: 0.14621299505233765\n",
      "349, L1: 0.14372661709785461, ssim: 0.2591249942779541\n",
      "350, L1: 0.07350350171327591, ssim: 0.0891922116279602\n",
      "351, L1: 0.06721048802137375, ssim: 0.08392554521560669\n",
      "352, L1: 0.10747326165437698, ssim: 0.3398524522781372\n",
      "353, L1: 0.10822968184947968, ssim: 0.10653716325759888\n",
      "354, L1: 0.12950129806995392, ssim: 0.167310893535614\n",
      "355, L1: 0.08349478989839554, ssim: 0.09803354740142822\n",
      "356, L1: 0.04080455005168915, ssim: 0.03385120630264282\n",
      "357, L1: 0.1121993139386177, ssim: 0.1372433304786682\n",
      "358, L1: 0.0764036774635315, ssim: 0.13210195302963257\n",
      "359, L1: 0.0626194104552269, ssim: 0.06759899854660034\n",
      "360, L1: 0.10065320134162903, ssim: 0.13973349332809448\n",
      "361, L1: 0.059108633548021317, ssim: 0.05231523513793945\n",
      "362, L1: 0.05279482901096344, ssim: 0.057510316371917725\n",
      "363, L1: 0.07145919650793076, ssim: 0.0872449278831482\n",
      "364, L1: 0.1511601358652115, ssim: 0.26103484630584717\n",
      "365, L1: 0.07901269197463989, ssim: 0.08134406805038452\n",
      "366, L1: 0.09542105346918106, ssim: 0.09233766794204712\n",
      "367, L1: 0.10027303546667099, ssim: 0.10056793689727783\n",
      "368, L1: 0.06160132586956024, ssim: 0.0661047101020813\n",
      "369, L1: 0.07238725572824478, ssim: 0.09948033094406128\n",
      "370, L1: 0.039646465331315994, ssim: 0.034636616706848145\n",
      "371, L1: 0.045067641884088516, ssim: 0.04577082395553589\n",
      "372, L1: 0.08162429183721542, ssim: 0.08977818489074707\n",
      "373, L1: 0.09139613062143326, ssim: 0.13064569234848022\n",
      "374, L1: 0.05801418423652649, ssim: 0.07546716928482056\n",
      "375, L1: 0.28945600986480713, ssim: 0.42257970571517944\n",
      "376, L1: 0.06727849692106247, ssim: 0.08149504661560059\n",
      "377, L1: 0.0883718878030777, ssim: 0.11213892698287964\n",
      "378, L1: 0.08253313601016998, ssim: 0.0955933928489685\n",
      "379, L1: 0.08850421756505966, ssim: 0.18149149417877197\n",
      "380, L1: 0.0716560035943985, ssim: 0.08304160833358765\n",
      "381, L1: 0.04840268939733505, ssim: 0.047740280628204346\n",
      "382, L1: 0.07801271975040436, ssim: 0.09658384323120117\n",
      "383, L1: 0.14100122451782227, ssim: 0.23104149103164673\n",
      "384, L1: 0.05557733029127121, ssim: 0.040375471115112305\n",
      "385, L1: 0.0734013020992279, ssim: 0.09148257970809937\n",
      "386, L1: 0.07535459101200104, ssim: 0.07645905017852783\n",
      "387, L1: 0.10910212993621826, ssim: 0.1773090958595276\n",
      "388, L1: 0.08278664946556091, ssim: 0.09421312808990479\n",
      "389, L1: 0.04585019126534462, ssim: 0.035433053970336914\n",
      "390, L1: 0.059462107717990875, ssim: 0.05533939599990845\n",
      "391, L1: 0.047230981290340424, ssim: 0.04450654983520508\n",
      "392, L1: 0.058597907423973083, ssim: 0.05782449245452881\n",
      "393, L1: 0.06294438242912292, ssim: 0.052165985107421875\n",
      "394, L1: 0.11204992979764938, ssim: 0.13766902685165405\n",
      "395, L1: 0.07768707722425461, ssim: 0.08835780620574951\n",
      "396, L1: 0.04935361072421074, ssim: 0.045519888401031494\n",
      "397, L1: 0.07260248810052872, ssim: 0.09945082664489746\n",
      "398, L1: 0.06350629031658173, ssim: 0.060149431228637695\n",
      "399, L1: 0.029037468135356903, ssim: 0.04249697923660278\n",
      "400, L1: 0.10416368395090103, ssim: 0.12763828039169312\n",
      "401, L1: 0.05912933871150017, ssim: 0.06332641839981079\n",
      "402, L1: 0.1044042780995369, ssim: 0.13709867000579834\n",
      "403, L1: 0.04507708176970482, ssim: 0.043595075607299805\n",
      "404, L1: 0.08836686611175537, ssim: 0.11108279228210449\n",
      "405, L1: 0.09578574448823929, ssim: 0.09913557767868042\n",
      "406, L1: 0.06555214524269104, ssim: 0.08723604679107666\n",
      "407, L1: 0.09289787709712982, ssim: 0.1369551420211792\n",
      "408, L1: 0.08672159165143967, ssim: 0.08862358331680298\n",
      "409, L1: 0.05351207032799721, ssim: 0.05977386236190796\n",
      "410, L1: 0.07382124662399292, ssim: 0.07484465837478638\n",
      "411, L1: 0.08638101816177368, ssim: 0.09512734413146973\n",
      "412, L1: 0.07211953401565552, ssim: 0.089447021484375\n",
      "413, L1: 0.10461785644292831, ssim: 0.15763872861862183\n",
      "414, L1: 0.07798909395933151, ssim: 0.08803015947341919\n",
      "415, L1: 0.07130032032728195, ssim: 0.0686408281326294\n",
      "416, L1: 0.08857698738574982, ssim: 0.09899061918258667\n",
      "417, L1: 0.17354293167591095, ssim: 0.2880939841270447\n",
      "418, L1: 0.11960969120264053, ssim: 0.19355911016464233\n",
      "419, L1: 0.08514861762523651, ssim: 0.08370321989059448\n",
      "420, L1: 0.06947579979896545, ssim: 0.07217586040496826\n",
      "421, L1: 0.06037680804729462, ssim: 0.06290340423583984\n",
      "422, L1: 0.04072794318199158, ssim: 0.04570603370666504\n",
      "423, L1: 0.07109180092811584, ssim: 0.06666749715805054\n",
      "424, L1: 0.0733092650771141, ssim: 0.08349400758743286\n",
      "425, L1: 0.0822066068649292, ssim: 0.0881226658821106\n",
      "426, L1: 0.17587202787399292, ssim: 0.2430676817893982\n",
      "427, L1: 0.05033344030380249, ssim: 0.04778015613555908\n",
      "428, L1: 0.054029833525419235, ssim: 0.06913012266159058\n",
      "429, L1: 0.08343128114938736, ssim: 0.09239643812179565\n",
      "430, L1: 0.06100674346089363, ssim: 0.05649751424789429\n",
      "431, L1: 0.08579210937023163, ssim: 0.10124492645263672\n",
      "432, L1: 0.1382099688053131, ssim: 0.17529475688934326\n",
      "433, L1: 0.21239972114562988, ssim: 0.26901644468307495\n",
      "434, L1: 0.06704381108283997, ssim: 0.1258183717727661\n",
      "435, L1: 0.08753770589828491, ssim: 0.10817992687225342\n",
      "436, L1: 0.07924285531044006, ssim: 0.08583372831344604\n",
      "437, L1: 0.08269406110048294, ssim: 0.21679162979125977\n",
      "438, L1: 0.059649527072906494, ssim: 0.06901490688323975\n",
      "439, L1: 0.10748173296451569, ssim: 0.2108595371246338\n",
      "440, L1: 0.1342848241329193, ssim: 0.15510433912277222\n",
      "441, L1: 0.06840657442808151, ssim: 0.08009451627731323\n",
      "442, L1: 0.09412024170160294, ssim: 0.11647439002990723\n",
      "443, L1: 0.05810222402215004, ssim: 0.06361716985702515\n",
      "444, L1: 0.09269958734512329, ssim: 0.09400177001953125\n",
      "445, L1: 0.09392348676919937, ssim: 0.07839655876159668\n",
      "446, L1: 0.15384723246097565, ssim: 0.19296795129776\n",
      "447, L1: 0.048115409910678864, ssim: 0.03767669200897217\n",
      "448, L1: 0.06383567303419113, ssim: 0.06605678796768188\n",
      "449, L1: 0.09487182646989822, ssim: 0.099892258644104\n",
      "450, L1: 0.16707096993923187, ssim: 0.17319250106811523\n",
      "451, L1: 0.14442120492458344, ssim: 0.26129859685897827\n",
      "452, L1: 0.11849634349346161, ssim: 0.1555403470993042\n",
      "453, L1: 0.09029483795166016, ssim: 0.1749444603919983\n",
      "454, L1: 0.05242181196808815, ssim: 0.04992032051086426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455, L1: 0.07434134930372238, ssim: 0.0922250747680664\n",
      "456, L1: 0.09858784079551697, ssim: 0.18295514583587646\n",
      "457, L1: 0.17381393909454346, ssim: 0.2208053469657898\n",
      "458, L1: 0.13513469696044922, ssim: 0.15482217073440552\n",
      "459, L1: 0.08193998038768768, ssim: 0.09691119194030762\n",
      "460, L1: 0.10298686474561691, ssim: 0.1109459400177002\n",
      "461, L1: 0.07968256622552872, ssim: 0.06657242774963379\n",
      "462, L1: 0.059610363095998764, ssim: 0.059905290603637695\n",
      "463, L1: 0.06045956164598465, ssim: 0.06798720359802246\n",
      "464, L1: 0.19083385169506073, ssim: 0.24148380756378174\n",
      "465, L1: 0.045080821961164474, ssim: 0.03660166263580322\n",
      "466, L1: 0.11044272035360336, ssim: 0.12912142276763916\n",
      "467, L1: 0.1096285954117775, ssim: 0.11498904228210449\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-f7d01114e235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#evaluate accuracy using L1 loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_L1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_L1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-4d1f8ff258b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evaluate accuracy using L1 loss\n",
    "for i, data in enumerate(dataloader):\n",
    "    output = encdec(data['c1'])\n",
    "    truth = data['c2']\n",
    "    loss_L1 = criterion_L1(output, truth)\n",
    "    \n",
    "    output_norm = (output + 1) / 2\n",
    "    truth_norm = (truth+1) / 2\n",
    "    loss_ssim = 1 - criterion_ssim(output_norm, truth_norm)\n",
    "    \n",
    "    print(f'{i}, L1: {loss_L1}, ssim: {loss_ssim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
