{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_msssim import MS_SSIM\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# TODO: make sure to .to(device) the class later, and also set up gpu\n",
    "\n",
    "# path to font list\n",
    "fonts_csv = \"fonts.csv\"\n",
    "# root directory for dataset\n",
    "dataroot = \"images\"\n",
    "# number of workers for dataloader\n",
    "workers = 0\n",
    "# number of epochs\n",
    "num_epochs = 25\n",
    "# batch size for training\n",
    "batch_size = 16\n",
    "# height and width of input image\n",
    "img_size = 64\n",
    "# the alphabet characters\n",
    "alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "# number of channels\n",
    "nc0 = 1 * len(alphabet)\n",
    "nc1 = 4 * len(alphabet)\n",
    "nc2 = 8 * len(alphabet)\n",
    "nc3 = 16 * len(alphabet)\n",
    "# disciminator channels\n",
    "dc0 = 1\n",
    "dc1 = 8\n",
    "dc2 = 16\n",
    "dc3 = 32\n",
    "# threshold\n",
    "thresh = 0\n",
    "# learning rate\n",
    "lr = 0.002\n",
    "# beta1 for Adam\n",
    "beta1 = 0.5\n",
    "# real label\n",
    "real_label = 1.0\n",
    "# fake label\n",
    "fake_label = 0.0\n",
    "# number of extra times to run the discriminator than the encdec per epoch\n",
    "num_dis = 1\n",
    "# coefficient of the discriminator loss in training\n",
    "cof_dis = 7e-3\n",
    "# number of patches to sample\n",
    "num_patches = 3\n",
    "# letter we use to generate all the other letters\n",
    "base_letter = 'R'\n",
    "# letter we are trying to generate\n",
    "gen_letter = 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FontDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.fontlist = pd.read_csv(csv_file, sep=' ')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fontlist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {}\n",
    "        for c in alphabet:\n",
    "            path = os.path.join(self.root_dir, c, f'{idx}.npy')\n",
    "            img = np.load(path)\n",
    "            img = img[img_size//2:img_size//2 + img_size, img_size//2:img_size//2 + img_size, :]\n",
    "            img = self.transform(img)\n",
    "            sample[c] = img\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://discuss.pytorch.org/t/is-there-anyway-to-do-gaussian-filtering-for-an-image-2d-3d-in-pytorch/12351/3\n",
    "def get_gaussian_kernel(kernel_size=3, sigma=2, channels=3):\n",
    "    # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)\n",
    "    x_coord = torch.arange(kernel_size)\n",
    "    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)\n",
    "    y_grid = x_grid.t()\n",
    "    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "\n",
    "    mean = (kernel_size - 1)/2.\n",
    "    variance = sigma**2.\n",
    "\n",
    "    # Calculate the 2-dimensional gaussian kernel which is\n",
    "    # the product of two gaussian distributions for two different\n",
    "    # variables (in this case called x and y)\n",
    "    gaussian_kernel = (1./(2.*math.pi*variance)) *\\\n",
    "                      torch.exp(\n",
    "                          -torch.sum((xy_grid - mean)**2., dim=-1) /\\\n",
    "                          (2*variance)\n",
    "                      )\n",
    "\n",
    "    # Make sure sum of values in gaussian kernel equals 1.\n",
    "    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "\n",
    "    # Reshape to 2d depthwise convolutional weight\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1)\n",
    "\n",
    "    gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels,\n",
    "                                kernel_size=kernel_size, groups=channels, bias=False, padding=1)\n",
    "\n",
    "    gaussian_filter.weight.data = gaussian_kernel\n",
    "    gaussian_filter.weight.requires_grad = False\n",
    "    \n",
    "    return gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.conv01 = nn.Conv2d(dc0, dc1, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(dc1, dc2, 3, padding=1)\n",
    "        self.conv23 = nn.Conv2d(dc2, dc3, 3, padding=1)\n",
    "#         self.conv24 = nn.Conv2d(dc2, dc4, 3, padding=1)\n",
    "#         self.conv34 = nn.Conv2d(dc3, dc4, 3, padding=1)\n",
    "        \n",
    "        self.conv33 = nn.Conv2d(dc3, dc3, 3, padding=1)\n",
    "        self.conv32 = nn.Conv2d(dc3, nc0, 3, padding=1)\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(nc0, nc1, 3, padding=1)\n",
    "        self.conv0same = nn.Conv2d(nc0, nc0, 3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(nc1, nc2, 3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(nc2, nc2, 3, padding=1)\n",
    "#         self.conv2back = nn.Conv2d(nc2, nc1, 3, padding=1)\n",
    "#         self.conv1back = nn.Conv2d(nc1, nc0, 3, padding=1)\n",
    "        \n",
    "#         self.conv1strided = nn.Conv2d(nc1, nc1, 3, stride=2, padding=1)\n",
    "#         self.conv2strided = nn.Conv2d(nc2, nc2, 3, stride=2, padding=1)\n",
    "\n",
    "#         self.deconv1 = nn.ConvTranspose2d(nc1, nc0, 3, padding=1)\n",
    "#         self.deconv2 = nn.ConvTranspose2d(nc2, nc1, 3, padding=1)\n",
    "#         self.deconv1strided = nn.ConvTranspose2d(nc1, nc1, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.deconv2strided = nn.ConvTranspose2d(nc2, nc2, 3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "#         self.batchnorm0 = nn.BatchNorm2d(nc0)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(nc1)\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(nc2)\n",
    "        self.pool = nn.MaxPool2d(2, return_indices=True)\n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.threshold = nn.Threshold(thresh, 0)\n",
    "        \n",
    "        self.gaussian_filter = get_gaussian_kernel(kernel_size = 3, sigma=2, channels=nc0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv01(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv23(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x, idx1 = self.pool(x)\n",
    "        \n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x, idx2 = self.pool(x)\n",
    "        \n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x, idx3 = self.pool(x)\n",
    "        \n",
    "        x = self.unpool(x, idx3)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        x = self.unpool(x, idx2)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        x = self.unpool(x, idx1)\n",
    "        x = self.conv32(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        x = self.gaussian_filter(x)\n",
    "        x = self.conv0same(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.conv0same(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_EncDec(nn.Module):\n",
    "    def __init__(self, nhead=8, num_encoder_layers=6):\n",
    "        model = nn.Transformer(d_model=img_size**2, nhead=nhead, num_encoder_layers=num_encoder_layers)\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.float().masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        trans.torch.ones((1, src.shape[0], img_size*img_size))\n",
    "        for i in src.shape[0]:\n",
    "            trans[0][i] = src[i][0].reshape(-1)\n",
    "        x = model(trans, src_mask)\n",
    "        \n",
    "        return model(src, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(nc0, nc2, 3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2stri = nn.Conv2d(nc2, nc2, 4, stride=2, padding=1)\n",
    "        self.conv2same = nn.Conv2d(nc2, nc2, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(nc2, nc3, 4, stride=2, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(nc3)\n",
    "        self.conv3same = nn.Conv2d(nc3, nc3, 3, padding=1)\n",
    "        self.conv3back = nn.Conv2d(nc3, nc2, 3, padding=1)\n",
    "        self.conv0back = nn.Conv2d(nc2, nc0, 3, padding=1)\n",
    "        self.flat = nn.Flatten(2)\n",
    "        self.linear = nn.Linear(img_size//8 * img_size//8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.flat1 = nn.Flatten()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # nc0 x img_size x img_size\n",
    "        out = self.conv0(input)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc2 x img_size x img_size\n",
    "        out = self.conv2stri(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc2 x img_size/2 x img_size/2\n",
    "        out = self.conv2stri(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc2 x img_size/4 x img_size/4\n",
    "        out = self.conv2same(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2same(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc3 x img_size/8 x img_size/8\n",
    "        out = self.conv3same(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3back(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # nc2 x img_size/8 x img_size/8\n",
    "        out = self.conv0back(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "#         print(out.size())\n",
    "        out = self.flat(out)\n",
    "#         print(out.size())\n",
    "        out = self.linear(out)\n",
    "#         print(out.size())\n",
    "        out = self.flat1(out)\n",
    "        out = self.sigmoid(out)\n",
    "#         print(out.size())\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 1 x img_size x img_size\n",
    "            nn.Conv2d(nc0, nc2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(nc2, nc2, 4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(nc1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 4 x img_size/2 x img_size/2\n",
    "            nn.Conv2d(nc2, nc2, 4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(nc2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(nc2, nc2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nc2, nc2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 8 x img_size/4 x img_size/4\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(nc2 * img_size // 4 * img_size // 4, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tensors(data):\n",
    "    tensor = None\n",
    "    \n",
    "    for c in alphabet:\n",
    "        if tensor == None:\n",
    "            tensor = data[c]\n",
    "        else:\n",
    "            tensor = torch.cat((tensor, data[c]), 1)\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossD_list = []\n",
    "loss_disc_list = []\n",
    "loss_l1_list = []\n",
    "loss_ssim_list = []\n",
    "loss_local_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset = FontDataset(csv_file=fonts_csv, \n",
    "                        root_dir=dataroot, \n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(0.5, 0.5),\n",
    "#                             AddGaussianNoise(0., 0.05),\n",
    "                        ]))\n",
    "    \n",
    "    testset_size = len(dataset) // 5 * 4\n",
    "    train_set, val_set = random_split(dataset, [testset_size, len(dataset) - testset_size], generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    train_data = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "    \n",
    "    encdec = EncoderDecoder()\n",
    "    # use this line to continue training instead of starting a new one\n",
    "    # encdec.load_state_dict(torch.load('encdec.pt'))\n",
    "    \n",
    "    criterionED_l1 = nn.L1Loss()\n",
    "    criterionED_ssim = MS_SSIM(win_size=3, data_range=1, size_average=True, channel=nc0)\n",
    "    optimizerED = optim.Adam(encdec.parameters(), lr=lr)\n",
    "\n",
    "    disc = Discriminator()\n",
    "    criterionD = nn.BCELoss()\n",
    "    optimizerD = optim.Adam(disc.parameters(), lr=lr)\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_lossD = 0.0\n",
    "        running_loss_disc = 0.0\n",
    "        running_loss_l1 = 0.0\n",
    "        running_loss_ssim = 0.0\n",
    "        running_loss_local = 0.0\n",
    "        \n",
    "#         for it in range(num_dis):\n",
    "#             for i, data in enumerate(train_data):\n",
    "#                 train_batch = concat_tensors(data)\n",
    "#                 disc.zero_grad()\n",
    "                \n",
    "#                 b_size = train_batch.size(0)\n",
    "#                 c_size = train_batch.size(1)\n",
    "#                 label = torch.full((b_size,c_size,), real_label, dtype=torch.float, device=device)\n",
    "                \n",
    "#                 outputD = disc(train_batch)\n",
    "#                 lossD_real = criterionD(outputD, label)\n",
    "#                 lossD_real.backward()\n",
    "\n",
    "#                 # all fake batch\n",
    "#                 outputED = encdec(train_batch)\n",
    "#                 label.fill_(fake_label)\n",
    "#                 outputD = disc(outputED.detach())\n",
    "#                 lossD_fake = criterionD(outputD, label)\n",
    "#                 lossD_fake.backward()\n",
    "\n",
    "#                 lossD = lossD_real + lossD_fake\n",
    "#                 optimizerD.step()\n",
    "#         print('finished updating dis')\n",
    "                \n",
    "        for i, data in enumerate(train_data):\n",
    "            train_batch = concat_tensors(data)\n",
    "            ###########################\n",
    "            # update disc\n",
    "            ###########################\n",
    "            disc.zero_grad()\n",
    "            # all real batch\n",
    "            b_size = train_batch.size(0)\n",
    "            c_size = train_batch.size(1)\n",
    "            label = torch.full((b_size,c_size,), real_label, dtype=torch.float, device=device)\n",
    "            outputD = disc(train_batch)\n",
    "            lossD_real = criterionD(outputD, label)\n",
    "            lossD_real.backward()\n",
    "            \n",
    "            # all fake batch\n",
    "            outputED = encdec(data[base_letter])\n",
    "            \n",
    "            label.fill_(fake_label)\n",
    "            outputD = disc(outputED.detach())\n",
    "            lossD_fake = criterionD(outputD, label)\n",
    "            lossD_fake.backward()\n",
    "\n",
    "            lossD = lossD_real + lossD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            ###########################\n",
    "            # update encdec\n",
    "            ###########################\n",
    "            encdec.zero_grad()\n",
    "            # rerun disc\n",
    "            label.fill_(real_label)\n",
    "            outputD = disc(outputED)\n",
    "            lossED_disc = criterionD(outputD, label)\n",
    "            # run encdec\n",
    "            lossED_l1 = criterionED_l1(outputED, train_batch)\n",
    "            \n",
    "            # calculate local loss\n",
    "            lossED_local = 0.0\n",
    "            for _ in range(num_patches):\n",
    "                x = np.random.randint(16, 48)\n",
    "                y = np.random.randint(16, 48)\n",
    "                for j, c in enumerate(alphabet):\n",
    "                    outputED_patch = outputED[:, j, x-8:x+8, y-8:y+8]\n",
    "                    datac2_patch = data[c][:, 0, x-8:x+8, y-8:y+8]\n",
    "                    outputED_patch = (outputED_patch + 1) / 2\n",
    "                    datac2_patch = (datac2_patch + 1) / 2\n",
    "                    lossED_local += criterionED_l1(outputED_patch, datac2_patch)\n",
    "            \n",
    "            outputED_norm = (outputED + 1) / 2\n",
    "            train_norm = (train_batch + 1) / 2\n",
    "            lossED_ssim = 1 - criterionED_ssim(outputED_norm, train_norm)\n",
    "\n",
    "            lossED = cof_dis * lossED_disc + 0.16 * lossED_l1 + 0.84 * lossED_ssim + lossED_local/num_patches\n",
    "            lossED.backward()\n",
    "            optimizerED.step()\n",
    "    \n",
    "            running_lossD += lossD.item()\n",
    "            running_loss_disc += lossED_disc.item()\n",
    "            running_loss_l1 += lossED_l1.item()\n",
    "            running_loss_ssim += lossED_ssim.item()\n",
    "            running_loss_local += lossED_local.item()\n",
    "            if i % 2 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Iteration {i+1}, Loss D {running_lossD}, Loss Disc {running_loss_disc}, Loss L1 {running_loss_l1}, Loss SSIM {running_loss_ssim}, Loss Local {running_loss_local}\")\n",
    "                lossD_list.append(running_lossD)\n",
    "                loss_disc_list.append(running_loss_disc)\n",
    "                loss_l1_list.append(running_loss_l1)\n",
    "                loss_ssim_list.append(running_loss_ssim)\n",
    "                loss_local_list.append(running_loss_local)\n",
    "                running_lossD = 0.0\n",
    "                running_loss_disc = 0.0\n",
    "                running_loss_l1 = 0.0\n",
    "                running_loss_ssim = 0.0\n",
    "                running_loss_local = 0.0\n",
    "                \n",
    "                fig = plt.figure(figsize=(8, 8))\n",
    "                fig.add_subplot(1, 2, 1)\n",
    "                plt.imshow(data['A'][0].permute(1, 2, 0).detach().numpy(), cmap='gray')\n",
    "                fig.add_subplot(1, 2, 2)\n",
    "                plt.imshow(outputED[0,0].detach().numpy(), cmap='gray')\n",
    "                plt.show()\n",
    "    \n",
    "#     torch.save(encdec.state_dict(), 'encdec.pt')\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Iteration 1, Loss D 1.3851085901260376, Loss Disc 0.48477256298065186, Loss L1 0.9970366954803467, Loss SSIM 0.677008330821991, Loss Local 37.27385330200195\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Iteration 3, Loss D 2.9028663635253906, Loss Disc 1.4869877099990845, Loss L1 1.9766608476638794, Loss SSIM 1.3944151997566223, Loss Local 72.5413589477539\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-404-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-403-7271793e1c57>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0moutputD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mlossD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterionD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mlossD_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# all fake batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(loss_ssim_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to font list\n",
    "fonts_csv = \"fonts.csv\"\n",
    "# root directory for dataset\n",
    "dataroot = \"images\"\n",
    "# number of workers for dataloader\n",
    "workers = 0\n",
    "\n",
    "dataset = FontDataset(csv_file=fonts_csv, \n",
    "                      root_dir=dataroot, \n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize(0.5, 0.5),\n",
    "                      ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=workers)\n",
    "\n",
    "encdec = EncoderDecoder()\n",
    "encdec.load_state_dict(torch.load('encdec.pt'))\n",
    "encdec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at result and ground truth side by side\n",
    "for i, data in enumerate(dataloader):\n",
    "    if i > 50:\n",
    "        break\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    output = encdec(data['c1'])\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(data['c2'][0].permute(1, 2, 0).detach().numpy(), cmap='gray')\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(output[0].permute(1, 2, 0).detach().numpy(), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up for evaluation\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=workers)\n",
    "\n",
    "encdec = EncoderDecoder()\n",
    "encdec.load_state_dict(torch.load('encdec-Copy1.pt'))\n",
    "encdec.eval()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "criterion_ssim = MS_SSIM(win_size=3, data_range=1, size_average=True, channel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, L1: 0.11039647459983826, ssim: 0.17747068405151367\n",
      "1, L1: 0.08089693635702133, ssim: 0.09386289119720459\n",
      "2, L1: 0.06853260844945908, ssim: 0.08297383785247803\n",
      "3, L1: 0.11089372634887695, ssim: 0.23213541507720947\n",
      "4, L1: 0.038106516003608704, ssim: 0.027527153491973877\n",
      "5, L1: 0.0792616680264473, ssim: 0.11540257930755615\n",
      "6, L1: 0.04213376343250275, ssim: 0.03352046012878418\n",
      "7, L1: 0.059157781302928925, ssim: 0.07703202962875366\n",
      "8, L1: 0.24022439122200012, ssim: 0.4588257670402527\n",
      "9, L1: 0.06825708597898483, ssim: 0.07667970657348633\n",
      "10, L1: 0.051598817110061646, ssim: 0.051001548767089844\n",
      "11, L1: 0.2085886001586914, ssim: 0.2567775249481201\n",
      "12, L1: 0.07073262333869934, ssim: 0.0784960389137268\n",
      "13, L1: 0.08040574193000793, ssim: 0.09605830907821655\n",
      "14, L1: 0.05451987311244011, ssim: 0.058758437633514404\n",
      "15, L1: 0.08923070132732391, ssim: 0.15772056579589844\n",
      "16, L1: 0.11131218075752258, ssim: 0.13691407442092896\n",
      "17, L1: 0.2438725084066391, ssim: 0.4737599492073059\n",
      "18, L1: 0.08858506381511688, ssim: 0.10257601737976074\n",
      "19, L1: 0.07195201516151428, ssim: 0.0875704288482666\n",
      "20, L1: 0.09331734478473663, ssim: 0.09679895639419556\n",
      "21, L1: 0.07823194563388824, ssim: 0.08822083473205566\n",
      "22, L1: 0.11739818751811981, ssim: 0.11387735605239868\n",
      "23, L1: 0.0667552649974823, ssim: 0.06381601095199585\n",
      "24, L1: 0.11998876929283142, ssim: 0.17088258266448975\n",
      "25, L1: 0.07983861118555069, ssim: 0.08174818754196167\n",
      "26, L1: 0.0639478787779808, ssim: 0.0803343653678894\n",
      "27, L1: 0.06877158582210541, ssim: 0.07201248407363892\n",
      "28, L1: 0.057810574769973755, ssim: 0.053766727447509766\n",
      "29, L1: 0.06933445483446121, ssim: 0.06888890266418457\n",
      "30, L1: 0.07556630671024323, ssim: 0.07531929016113281\n",
      "31, L1: 0.08168505132198334, ssim: 0.12882930040359497\n",
      "32, L1: 0.07539597153663635, ssim: 0.09400874376296997\n",
      "33, L1: 0.10875514894723892, ssim: 0.14444255828857422\n",
      "34, L1: 0.10287395119667053, ssim: 0.11146926879882812\n",
      "35, L1: 0.060638319700956345, ssim: 0.06325531005859375\n",
      "36, L1: 0.06531141698360443, ssim: 0.08455002307891846\n",
      "37, L1: 0.06041226163506508, ssim: 0.059891700744628906\n",
      "38, L1: 0.11109914630651474, ssim: 0.16576504707336426\n",
      "39, L1: 0.0741693526506424, ssim: 0.08079296350479126\n",
      "40, L1: 0.08219931274652481, ssim: 0.09181523323059082\n",
      "41, L1: 0.04982729256153107, ssim: 0.07389497756958008\n",
      "42, L1: 0.07869037240743637, ssim: 0.142287015914917\n",
      "43, L1: 0.08044759184122086, ssim: 0.09641438722610474\n",
      "44, L1: 0.0699697807431221, ssim: 0.07415330410003662\n",
      "45, L1: 0.086062952876091, ssim: 0.0865662693977356\n",
      "46, L1: 0.059649527072906494, ssim: 0.06901490688323975\n",
      "47, L1: 0.05411982536315918, ssim: 0.05727654695510864\n",
      "48, L1: 0.06486041098833084, ssim: 0.058390676975250244\n",
      "49, L1: 0.06844756752252579, ssim: 0.09703141450881958\n",
      "50, L1: 0.07016517221927643, ssim: 0.07640033960342407\n",
      "51, L1: 0.07472244650125504, ssim: 0.08153796195983887\n",
      "52, L1: 0.051572803407907486, ssim: 0.05117243528366089\n",
      "53, L1: 0.07311668246984482, ssim: 0.08347994089126587\n",
      "54, L1: 0.08789613097906113, ssim: 0.10204607248306274\n",
      "55, L1: 0.08777797967195511, ssim: 0.21949148178100586\n",
      "56, L1: 0.05837231129407883, ssim: 0.06220120191574097\n",
      "57, L1: 0.09302449226379395, ssim: 0.1497597098350525\n",
      "58, L1: 0.063418447971344, ssim: 0.05179637670516968\n",
      "59, L1: 0.06547600030899048, ssim: 0.2235812544822693\n",
      "60, L1: 0.16570019721984863, ssim: 0.24871176481246948\n",
      "61, L1: 0.20832090079784393, ssim: 0.46981626749038696\n",
      "62, L1: 0.08657428622245789, ssim: 0.11344945430755615\n",
      "63, L1: 0.06240950524806976, ssim: 0.06965649127960205\n",
      "64, L1: 0.04638919234275818, ssim: 0.03624379634857178\n",
      "65, L1: 0.08564560860395432, ssim: 0.10714036226272583\n",
      "66, L1: 0.05813151225447655, ssim: 0.06423807144165039\n",
      "67, L1: 0.10618391633033752, ssim: 0.1134800910949707\n",
      "68, L1: 0.16570019721984863, ssim: 0.24871176481246948\n",
      "69, L1: 0.08349545300006866, ssim: 0.09735465049743652\n",
      "70, L1: 0.058658674359321594, ssim: 0.06384211778640747\n",
      "71, L1: 0.07996442914009094, ssim: 0.0873371958732605\n",
      "72, L1: 0.08564764261245728, ssim: 0.11042255163192749\n",
      "73, L1: 0.06957830488681793, ssim: 0.06417578458786011\n",
      "74, L1: 0.0667349100112915, ssim: 0.08756035566329956\n",
      "75, L1: 0.060447514057159424, ssim: 0.0736168622970581\n",
      "76, L1: 0.11580699682235718, ssim: 0.14616823196411133\n",
      "77, L1: 0.08010882139205933, ssim: 0.09115850925445557\n",
      "78, L1: 0.06224963814020157, ssim: 0.07315593957901001\n",
      "79, L1: 0.0801122859120369, ssim: 0.08572971820831299\n",
      "80, L1: 0.05519719049334526, ssim: 0.05046182870864868\n",
      "81, L1: 0.04991481825709343, ssim: 0.0470157265663147\n",
      "82, L1: 0.08203059434890747, ssim: 0.09390407800674438\n",
      "83, L1: 0.08293889462947845, ssim: 0.12393331527709961\n",
      "84, L1: 0.042745523154735565, ssim: 0.047422826290130615\n",
      "85, L1: 0.05421433970332146, ssim: 0.042391836643218994\n",
      "86, L1: 0.09777764976024628, ssim: 0.11692667007446289\n",
      "87, L1: 0.16442610323429108, ssim: 0.18031913042068481\n",
      "88, L1: 0.05864734202623367, ssim: 0.05882197618484497\n",
      "89, L1: 0.21817171573638916, ssim: 0.26338082551956177\n",
      "90, L1: 0.073409304022789, ssim: 0.13550305366516113\n",
      "91, L1: 0.10923173278570175, ssim: 0.14106416702270508\n",
      "92, L1: 0.09580215811729431, ssim: 0.11222904920578003\n",
      "93, L1: 0.1138966977596283, ssim: 0.14607203006744385\n",
      "94, L1: 0.06607350707054138, ssim: 0.065890371799469\n",
      "95, L1: 0.040087275207042694, ssim: 0.030010640621185303\n",
      "96, L1: 0.05828620493412018, ssim: 0.05548572540283203\n",
      "97, L1: 0.09335895627737045, ssim: 0.1558029055595398\n",
      "98, L1: 0.05157656595110893, ssim: 0.04911792278289795\n",
      "99, L1: 0.2196563482284546, ssim: 0.2701200842857361\n",
      "100, L1: 0.0709017813205719, ssim: 0.19460755586624146\n",
      "101, L1: 0.07120824605226517, ssim: 0.0683140754699707\n",
      "102, L1: 0.10556203871965408, ssim: 0.14547669887542725\n",
      "103, L1: 0.036800943315029144, ssim: 0.026219844818115234\n",
      "104, L1: 0.07244245707988739, ssim: 0.07723045349121094\n",
      "105, L1: 0.363224059343338, ssim: 0.6219459772109985\n",
      "106, L1: 0.0718672052025795, ssim: 0.06966918706893921\n",
      "107, L1: 0.06915422528982162, ssim: 0.0712231993675232\n",
      "108, L1: 0.05729978159070015, ssim: 0.05964893102645874\n",
      "109, L1: 0.08596564829349518, ssim: 0.1078413724899292\n",
      "110, L1: 0.06418215483427048, ssim: 0.07358425855636597\n",
      "111, L1: 0.05040813237428665, ssim: 0.20286911725997925\n",
      "112, L1: 0.07725847512483597, ssim: 0.08804017305374146\n",
      "113, L1: 0.05448508635163307, ssim: 0.051821112632751465\n",
      "114, L1: 0.07702110707759857, ssim: 0.08476537466049194\n",
      "115, L1: 0.03965801000595093, ssim: 0.029412388801574707\n",
      "116, L1: 0.06490347534418106, ssim: 0.07555633783340454\n",
      "117, L1: 0.08618807792663574, ssim: 0.08942347764968872\n",
      "118, L1: 0.055978137999773026, ssim: 0.057775795459747314\n",
      "119, L1: 0.07456126809120178, ssim: 0.09267371892929077\n",
      "120, L1: 0.0553063228726387, ssim: 0.05444842576980591\n",
      "121, L1: 0.08147662878036499, ssim: 0.10801106691360474\n",
      "122, L1: 0.38869813084602356, ssim: 0.6513954401016235\n",
      "123, L1: 0.09291809052228928, ssim: 0.09210264682769775\n",
      "124, L1: 0.11574260890483856, ssim: 0.2515529990196228\n",
      "125, L1: 0.1037553921341896, ssim: 0.13993853330612183\n",
      "126, L1: 0.07121125608682632, ssim: 0.07420307397842407\n",
      "127, L1: 0.06316206604242325, ssim: 0.06434404850006104\n",
      "128, L1: 0.09860745072364807, ssim: 0.11562669277191162\n",
      "129, L1: 0.0841875672340393, ssim: 0.14181244373321533\n",
      "130, L1: 0.06615540385246277, ssim: 0.07668787240982056\n",
      "131, L1: 0.0925292894244194, ssim: 0.09582686424255371\n",
      "132, L1: 0.0801507979631424, ssim: 0.07964849472045898\n",
      "133, L1: 0.33368998765945435, ssim: 0.4384918808937073\n",
      "134, L1: 0.08694475144147873, ssim: 0.10211265087127686\n",
      "135, L1: 0.06750816106796265, ssim: 0.06944459676742554\n",
      "136, L1: 0.0782369077205658, ssim: 0.09451031684875488\n",
      "137, L1: 0.11510879546403885, ssim: 0.13581758737564087\n",
      "138, L1: 0.0757659524679184, ssim: 0.09976613521575928\n",
      "139, L1: 0.04973413050174713, ssim: 0.11817502975463867\n",
      "140, L1: 0.11960969120264053, ssim: 0.19355911016464233\n",
      "141, L1: 0.05052134394645691, ssim: 0.059031009674072266\n",
      "142, L1: 0.058373790234327316, ssim: 0.06399023532867432\n",
      "143, L1: 0.06849811226129532, ssim: 0.09964889287948608\n",
      "144, L1: 0.04926455765962601, ssim: 0.04742705821990967\n",
      "145, L1: 0.23615115880966187, ssim: 0.42386651039123535\n",
      "146, L1: 0.07050785422325134, ssim: 0.07862329483032227\n",
      "147, L1: 0.07191720604896545, ssim: 0.1325216293334961\n",
      "148, L1: 0.049936674535274506, ssim: 0.046259522438049316\n",
      "149, L1: 0.07645054906606674, ssim: 0.10430055856704712\n",
      "150, L1: 0.09542341530323029, ssim: 0.09699338674545288\n",
      "151, L1: 0.09087681770324707, ssim: 0.1399936079978943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152, L1: 0.06889532506465912, ssim: 0.06919312477111816\n",
      "153, L1: 0.0607423260807991, ssim: 0.06934094429016113\n",
      "154, L1: 0.05836329609155655, ssim: 0.06114429235458374\n",
      "155, L1: 0.08883416652679443, ssim: 0.1027938723564148\n",
      "156, L1: 0.07575616985559464, ssim: 0.0858493447303772\n",
      "157, L1: 0.07978146523237228, ssim: 0.10669815540313721\n",
      "158, L1: 0.05258508026599884, ssim: 0.05345994234085083\n",
      "159, L1: 0.05873097851872444, ssim: 0.05718791484832764\n",
      "160, L1: 0.07342013716697693, ssim: 0.08319908380508423\n",
      "161, L1: 0.07586877793073654, ssim: 0.09458595514297485\n",
      "162, L1: 0.09478834271430969, ssim: 0.1022070050239563\n",
      "163, L1: 0.10470055043697357, ssim: 0.22193455696105957\n",
      "164, L1: 0.08443177491426468, ssim: 0.08526414632797241\n",
      "165, L1: 0.0715842917561531, ssim: 0.0762944221496582\n",
      "166, L1: 0.059848956763744354, ssim: 0.06284481287002563\n",
      "167, L1: 0.09003887325525284, ssim: 0.10121101140975952\n",
      "168, L1: 0.07221962511539459, ssim: 0.08247703313827515\n",
      "169, L1: 0.05456048250198364, ssim: 0.05774599313735962\n",
      "170, L1: 0.062319912016391754, ssim: 0.061666131019592285\n",
      "171, L1: 0.12235996127128601, ssim: 0.14731186628341675\n",
      "172, L1: 0.07585728913545609, ssim: 0.0846605896949768\n",
      "173, L1: 0.08897440880537033, ssim: 0.08552366495132446\n",
      "174, L1: 0.09595345705747604, ssim: 0.11795312166213989\n",
      "175, L1: 0.1530618667602539, ssim: 0.20418328046798706\n",
      "176, L1: 0.06844436377286911, ssim: 0.07597720623016357\n",
      "177, L1: 0.05680049955844879, ssim: 0.05834805965423584\n",
      "178, L1: 0.042479176074266434, ssim: 0.047439396381378174\n",
      "179, L1: 0.11444111168384552, ssim: 0.14104092121124268\n",
      "180, L1: 0.0944470688700676, ssim: 0.09846585988998413\n",
      "181, L1: 0.08431929349899292, ssim: 0.09580868482589722\n",
      "182, L1: 0.0876365378499031, ssim: 0.11429256200790405\n",
      "183, L1: 0.057242587208747864, ssim: 0.06606119871139526\n",
      "184, L1: 0.05481316149234772, ssim: 0.054457783699035645\n",
      "185, L1: 0.0776325911283493, ssim: 0.10221612453460693\n",
      "186, L1: 0.06466228514909744, ssim: 0.20862966775894165\n",
      "187, L1: 0.06182638183236122, ssim: 0.06469154357910156\n",
      "188, L1: 0.0865883082151413, ssim: 0.1480005383491516\n",
      "189, L1: 0.07957552373409271, ssim: 0.10169345140457153\n",
      "190, L1: 0.06658457964658737, ssim: 0.0711585283279419\n",
      "191, L1: 0.0835801213979721, ssim: 0.12852948904037476\n",
      "192, L1: 0.08956162631511688, ssim: 0.11507654190063477\n",
      "193, L1: 0.08366426080465317, ssim: 0.08775860071182251\n",
      "194, L1: 0.07963170111179352, ssim: 0.09295207262039185\n",
      "195, L1: 0.12010495364665985, ssim: 0.17237186431884766\n",
      "196, L1: 0.07515397667884827, ssim: 0.06803417205810547\n",
      "197, L1: 0.08823316544294357, ssim: 0.24441921710968018\n",
      "198, L1: 0.1182071790099144, ssim: 0.18135285377502441\n",
      "199, L1: 0.04738513380289078, ssim: 0.044183433055877686\n",
      "200, L1: 0.060185059905052185, ssim: 0.14968520402908325\n",
      "201, L1: 0.06966700404882431, ssim: 0.08720976114273071\n",
      "202, L1: 0.09440502524375916, ssim: 0.10351788997650146\n",
      "203, L1: 0.18186888098716736, ssim: 0.2592860460281372\n",
      "204, L1: 0.04172550514340401, ssim: 0.03743618726730347\n",
      "205, L1: 0.053161703050136566, ssim: 0.06206309795379639\n",
      "206, L1: 0.06942267715930939, ssim: 0.09048372507095337\n",
      "207, L1: 0.06128733232617378, ssim: 0.059324681758880615\n",
      "208, L1: 0.06123587489128113, ssim: 0.05832546949386597\n",
      "209, L1: 0.05999543145298958, ssim: 0.056419193744659424\n",
      "210, L1: 0.07286524772644043, ssim: 0.07574743032455444\n",
      "211, L1: 0.09657705575227737, ssim: 0.11037158966064453\n",
      "212, L1: 0.0864144042134285, ssim: 0.09380531311035156\n",
      "213, L1: 0.05756179243326187, ssim: 0.10089355707168579\n",
      "214, L1: 0.0947018563747406, ssim: 0.10982930660247803\n",
      "215, L1: 0.07384870946407318, ssim: 0.07740873098373413\n",
      "216, L1: 0.07900752127170563, ssim: 0.08479499816894531\n",
      "217, L1: 0.17347300052642822, ssim: 0.29384297132492065\n",
      "218, L1: 0.06532029807567596, ssim: 0.08351916074752808\n",
      "219, L1: 0.13060584664344788, ssim: 0.20194917917251587\n",
      "220, L1: 0.1389095038175583, ssim: 0.18415135145187378\n",
      "221, L1: 0.08162102848291397, ssim: 0.09310156106948853\n",
      "222, L1: 0.08462904393672943, ssim: 0.0932847261428833\n",
      "223, L1: 0.07755652070045471, ssim: 0.08016300201416016\n",
      "224, L1: 0.0725570023059845, ssim: 0.12490123510360718\n",
      "225, L1: 0.06201958283782005, ssim: 0.06326836347579956\n",
      "226, L1: 0.04686787724494934, ssim: 0.044676899909973145\n",
      "227, L1: 0.1690942943096161, ssim: 0.22202932834625244\n",
      "228, L1: 0.07996238023042679, ssim: 0.0848393440246582\n",
      "229, L1: 0.05495139956474304, ssim: 0.08629882335662842\n",
      "230, L1: 0.2052212804555893, ssim: 0.24015653133392334\n",
      "231, L1: 0.05728220194578171, ssim: 0.06035053730010986\n",
      "232, L1: 0.08465469628572464, ssim: 0.20122021436691284\n",
      "233, L1: 0.0871758908033371, ssim: 0.09999126195907593\n",
      "234, L1: 0.08425348252058029, ssim: 0.10168063640594482\n",
      "235, L1: 0.11844606697559357, ssim: 0.17073631286621094\n",
      "236, L1: 0.09476970881223679, ssim: 0.08210432529449463\n",
      "237, L1: 0.06849946081638336, ssim: 0.0633171796798706\n",
      "238, L1: 0.07332988828420639, ssim: 0.07410955429077148\n",
      "239, L1: 0.10579197108745575, ssim: 0.12645381689071655\n",
      "240, L1: 0.05388009920716286, ssim: 0.04750114679336548\n",
      "241, L1: 0.07468826323747635, ssim: 0.08048844337463379\n",
      "242, L1: 0.06616942584514618, ssim: 0.06644916534423828\n",
      "243, L1: 0.08787250518798828, ssim: 0.11075037717819214\n",
      "244, L1: 0.10416284203529358, ssim: 0.11556333303451538\n",
      "245, L1: 0.09403278678655624, ssim: 0.13193047046661377\n",
      "246, L1: 0.07840954512357712, ssim: 0.10303407907485962\n",
      "247, L1: 0.06790861487388611, ssim: 0.07219374179840088\n",
      "248, L1: 0.06619440019130707, ssim: 0.1070556640625\n",
      "249, L1: 0.16342364251613617, ssim: 0.21648216247558594\n",
      "250, L1: 0.05426584556698799, ssim: 0.05630171298980713\n",
      "251, L1: 0.2402423918247223, ssim: 0.32757794857025146\n",
      "252, L1: 0.07355061173439026, ssim: 0.08167952299118042\n",
      "253, L1: 0.06966373324394226, ssim: 0.07413965463638306\n",
      "254, L1: 0.05669628456234932, ssim: 0.05237644910812378\n",
      "255, L1: 0.058412693440914154, ssim: 0.051519572734832764\n",
      "256, L1: 0.097732774913311, ssim: 0.14348751306533813\n",
      "257, L1: 0.04665568843483925, ssim: 0.033098697662353516\n",
      "258, L1: 0.14095251262187958, ssim: 0.1595686674118042\n",
      "259, L1: 0.05420029163360596, ssim: 0.04765266180038452\n",
      "260, L1: 0.08288739621639252, ssim: 0.09040111303329468\n",
      "261, L1: 0.10381953418254852, ssim: 0.1591450572013855\n",
      "262, L1: 0.0835602730512619, ssim: 0.10077381134033203\n",
      "263, L1: 0.07832226157188416, ssim: 0.07500934600830078\n",
      "264, L1: 0.07392238825559616, ssim: 0.08819985389709473\n",
      "265, L1: 0.06708264350891113, ssim: 0.09616190195083618\n",
      "266, L1: 0.059720728546381, ssim: 0.062116384506225586\n",
      "267, L1: 0.09583042562007904, ssim: 0.11368948221206665\n",
      "268, L1: 0.06947579979896545, ssim: 0.07217586040496826\n",
      "269, L1: 0.06668949872255325, ssim: 0.07714807987213135\n",
      "270, L1: 0.07161005586385727, ssim: 0.08423900604248047\n",
      "271, L1: 0.07023369520902634, ssim: 0.07432836294174194\n",
      "272, L1: 0.08595754206180573, ssim: 0.09518271684646606\n",
      "273, L1: 0.10082989186048508, ssim: 0.12458795309066772\n",
      "274, L1: 0.048115409910678864, ssim: 0.03767669200897217\n",
      "275, L1: 0.0808941051363945, ssim: 0.07556438446044922\n",
      "276, L1: 0.12361115217208862, ssim: 0.11940431594848633\n",
      "277, L1: 0.06663946062326431, ssim: 0.0787515640258789\n",
      "278, L1: 0.16595958173274994, ssim: 0.2146252989768982\n",
      "279, L1: 0.06369791179895401, ssim: 0.05882817506790161\n",
      "280, L1: 0.05511406809091568, ssim: 0.05168342590332031\n",
      "281, L1: 0.08290721476078033, ssim: 0.09241241216659546\n",
      "282, L1: 0.1203908920288086, ssim: 0.22042924165725708\n",
      "283, L1: 0.1720588058233261, ssim: 0.2670275568962097\n",
      "284, L1: 0.05577537789940834, ssim: 0.06313908100128174\n",
      "285, L1: 0.07195201516151428, ssim: 0.0875704288482666\n",
      "286, L1: 0.07557158917188644, ssim: 0.0888986587524414\n",
      "287, L1: 0.06484436243772507, ssim: 0.055178046226501465\n",
      "288, L1: 0.08279614895582199, ssim: 0.07797175645828247\n",
      "289, L1: 0.11124784499406815, ssim: 0.11381286382675171\n",
      "290, L1: 0.14243760704994202, ssim: 0.17194491624832153\n",
      "291, L1: 0.10548247396945953, ssim: 0.17876434326171875\n",
      "292, L1: 0.08736469596624374, ssim: 0.09027671813964844\n",
      "293, L1: 0.12287268042564392, ssim: 0.15463876724243164\n",
      "294, L1: 0.0497937947511673, ssim: 0.050250113010406494\n",
      "295, L1: 0.059230878949165344, ssim: 0.11424052715301514\n",
      "296, L1: 0.08963367342948914, ssim: 0.12102240324020386\n",
      "297, L1: 0.1302875280380249, ssim: 0.18664777278900146\n",
      "298, L1: 0.08004625141620636, ssim: 0.07491379976272583\n",
      "299, L1: 0.07497795671224594, ssim: 0.07307237386703491\n",
      "300, L1: 0.0830988809466362, ssim: 0.08308237791061401\n",
      "301, L1: 0.09842289239168167, ssim: 0.14096713066101074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302, L1: 0.09000823646783829, ssim: 0.10436290502548218\n",
      "303, L1: 0.03278225660324097, ssim: 0.03346598148345947\n",
      "304, L1: 0.06608622521162033, ssim: 0.05992388725280762\n",
      "305, L1: 0.07300208508968353, ssim: 0.0842466950416565\n",
      "306, L1: 0.0675133690237999, ssim: 0.0772365927696228\n",
      "307, L1: 0.05704762414097786, ssim: 0.04869323968887329\n",
      "308, L1: 0.1096034049987793, ssim: 0.13312697410583496\n",
      "309, L1: 0.06016245484352112, ssim: 0.04968059062957764\n",
      "310, L1: 0.13053369522094727, ssim: 0.19141721725463867\n",
      "311, L1: 0.07232728600502014, ssim: 0.08200651407241821\n",
      "312, L1: 0.06184431537985802, ssim: 0.06264740228652954\n",
      "313, L1: 0.09022515267133713, ssim: 0.10453170537948608\n",
      "314, L1: 0.08760853856801987, ssim: 0.11121916770935059\n",
      "315, L1: 0.06120019033551216, ssim: 0.09021127223968506\n",
      "316, L1: 0.05880950391292572, ssim: 0.06488174200057983\n",
      "317, L1: 0.07695657014846802, ssim: 0.08762592077255249\n",
      "318, L1: 0.07153172791004181, ssim: 0.08104550838470459\n",
      "319, L1: 0.11937202513217926, ssim: 0.17387765645980835\n",
      "320, L1: 0.057829491794109344, ssim: 0.06199842691421509\n",
      "321, L1: 0.15086741745471954, ssim: 0.17904341220855713\n",
      "322, L1: 0.14659658074378967, ssim: 0.2676957845687866\n",
      "323, L1: 0.1499047577381134, ssim: 0.2714890241622925\n",
      "324, L1: 0.08321484923362732, ssim: 0.13656288385391235\n",
      "325, L1: 0.08886035531759262, ssim: 0.09996271133422852\n",
      "326, L1: 0.08896998316049576, ssim: 0.09781789779663086\n",
      "327, L1: 0.08234251290559769, ssim: 0.08661258220672607\n",
      "328, L1: 0.0607423260807991, ssim: 0.06934094429016113\n",
      "329, L1: 0.07111906260251999, ssim: 0.07233577966690063\n",
      "330, L1: 0.07067513465881348, ssim: 0.08010554313659668\n",
      "331, L1: 0.10411463677883148, ssim: 0.14877676963806152\n",
      "332, L1: 0.07902353256940842, ssim: 0.1579485535621643\n",
      "333, L1: 0.07486649602651596, ssim: 0.07890260219573975\n",
      "334, L1: 0.07320697605609894, ssim: 0.08896887302398682\n",
      "335, L1: 0.07104483991861343, ssim: 0.08805191516876221\n",
      "336, L1: 0.08816683292388916, ssim: 0.09256893396377563\n",
      "337, L1: 0.0623844712972641, ssim: 0.07120418548583984\n",
      "338, L1: 0.05893055349588394, ssim: 0.052982449531555176\n",
      "339, L1: 0.10152829438447952, ssim: 0.26605910062789917\n",
      "340, L1: 0.05734146386384964, ssim: 0.06710284948348999\n",
      "341, L1: 0.08044411242008209, ssim: 0.13162928819656372\n",
      "342, L1: 0.10540878027677536, ssim: 0.12518125772476196\n",
      "343, L1: 0.08080033957958221, ssim: 0.115223228931427\n",
      "344, L1: 0.07318978011608124, ssim: 0.08248287439346313\n",
      "345, L1: 0.08303725719451904, ssim: 0.09802478551864624\n",
      "346, L1: 0.05170012265443802, ssim: 0.04547780752182007\n",
      "347, L1: 0.2575848698616028, ssim: 0.35317373275756836\n",
      "348, L1: 0.11975828558206558, ssim: 0.14621299505233765\n",
      "349, L1: 0.14372661709785461, ssim: 0.2591249942779541\n",
      "350, L1: 0.07350350171327591, ssim: 0.0891922116279602\n",
      "351, L1: 0.06721048802137375, ssim: 0.08392554521560669\n",
      "352, L1: 0.10747326165437698, ssim: 0.3398524522781372\n",
      "353, L1: 0.10822968184947968, ssim: 0.10653716325759888\n",
      "354, L1: 0.12950129806995392, ssim: 0.167310893535614\n",
      "355, L1: 0.08349478989839554, ssim: 0.09803354740142822\n",
      "356, L1: 0.04080455005168915, ssim: 0.03385120630264282\n",
      "357, L1: 0.1121993139386177, ssim: 0.1372433304786682\n",
      "358, L1: 0.0764036774635315, ssim: 0.13210195302963257\n",
      "359, L1: 0.0626194104552269, ssim: 0.06759899854660034\n",
      "360, L1: 0.10065320134162903, ssim: 0.13973349332809448\n",
      "361, L1: 0.059108633548021317, ssim: 0.05231523513793945\n",
      "362, L1: 0.05279482901096344, ssim: 0.057510316371917725\n",
      "363, L1: 0.07145919650793076, ssim: 0.0872449278831482\n",
      "364, L1: 0.1511601358652115, ssim: 0.26103484630584717\n",
      "365, L1: 0.07901269197463989, ssim: 0.08134406805038452\n",
      "366, L1: 0.09542105346918106, ssim: 0.09233766794204712\n",
      "367, L1: 0.10027303546667099, ssim: 0.10056793689727783\n",
      "368, L1: 0.06160132586956024, ssim: 0.0661047101020813\n",
      "369, L1: 0.07238725572824478, ssim: 0.09948033094406128\n",
      "370, L1: 0.039646465331315994, ssim: 0.034636616706848145\n",
      "371, L1: 0.045067641884088516, ssim: 0.04577082395553589\n",
      "372, L1: 0.08162429183721542, ssim: 0.08977818489074707\n",
      "373, L1: 0.09139613062143326, ssim: 0.13064569234848022\n",
      "374, L1: 0.05801418423652649, ssim: 0.07546716928482056\n",
      "375, L1: 0.28945600986480713, ssim: 0.42257970571517944\n",
      "376, L1: 0.06727849692106247, ssim: 0.08149504661560059\n",
      "377, L1: 0.0883718878030777, ssim: 0.11213892698287964\n",
      "378, L1: 0.08253313601016998, ssim: 0.0955933928489685\n",
      "379, L1: 0.08850421756505966, ssim: 0.18149149417877197\n",
      "380, L1: 0.0716560035943985, ssim: 0.08304160833358765\n",
      "381, L1: 0.04840268939733505, ssim: 0.047740280628204346\n",
      "382, L1: 0.07801271975040436, ssim: 0.09658384323120117\n",
      "383, L1: 0.14100122451782227, ssim: 0.23104149103164673\n",
      "384, L1: 0.05557733029127121, ssim: 0.040375471115112305\n",
      "385, L1: 0.0734013020992279, ssim: 0.09148257970809937\n",
      "386, L1: 0.07535459101200104, ssim: 0.07645905017852783\n",
      "387, L1: 0.10910212993621826, ssim: 0.1773090958595276\n",
      "388, L1: 0.08278664946556091, ssim: 0.09421312808990479\n",
      "389, L1: 0.04585019126534462, ssim: 0.035433053970336914\n",
      "390, L1: 0.059462107717990875, ssim: 0.05533939599990845\n",
      "391, L1: 0.047230981290340424, ssim: 0.04450654983520508\n",
      "392, L1: 0.058597907423973083, ssim: 0.05782449245452881\n",
      "393, L1: 0.06294438242912292, ssim: 0.052165985107421875\n",
      "394, L1: 0.11204992979764938, ssim: 0.13766902685165405\n",
      "395, L1: 0.07768707722425461, ssim: 0.08835780620574951\n",
      "396, L1: 0.04935361072421074, ssim: 0.045519888401031494\n",
      "397, L1: 0.07260248810052872, ssim: 0.09945082664489746\n",
      "398, L1: 0.06350629031658173, ssim: 0.060149431228637695\n",
      "399, L1: 0.029037468135356903, ssim: 0.04249697923660278\n",
      "400, L1: 0.10416368395090103, ssim: 0.12763828039169312\n",
      "401, L1: 0.05912933871150017, ssim: 0.06332641839981079\n",
      "402, L1: 0.1044042780995369, ssim: 0.13709867000579834\n",
      "403, L1: 0.04507708176970482, ssim: 0.043595075607299805\n",
      "404, L1: 0.08836686611175537, ssim: 0.11108279228210449\n",
      "405, L1: 0.09578574448823929, ssim: 0.09913557767868042\n",
      "406, L1: 0.06555214524269104, ssim: 0.08723604679107666\n",
      "407, L1: 0.09289787709712982, ssim: 0.1369551420211792\n",
      "408, L1: 0.08672159165143967, ssim: 0.08862358331680298\n",
      "409, L1: 0.05351207032799721, ssim: 0.05977386236190796\n",
      "410, L1: 0.07382124662399292, ssim: 0.07484465837478638\n",
      "411, L1: 0.08638101816177368, ssim: 0.09512734413146973\n",
      "412, L1: 0.07211953401565552, ssim: 0.089447021484375\n",
      "413, L1: 0.10461785644292831, ssim: 0.15763872861862183\n",
      "414, L1: 0.07798909395933151, ssim: 0.08803015947341919\n",
      "415, L1: 0.07130032032728195, ssim: 0.0686408281326294\n",
      "416, L1: 0.08857698738574982, ssim: 0.09899061918258667\n",
      "417, L1: 0.17354293167591095, ssim: 0.2880939841270447\n",
      "418, L1: 0.11960969120264053, ssim: 0.19355911016464233\n",
      "419, L1: 0.08514861762523651, ssim: 0.08370321989059448\n",
      "420, L1: 0.06947579979896545, ssim: 0.07217586040496826\n",
      "421, L1: 0.06037680804729462, ssim: 0.06290340423583984\n",
      "422, L1: 0.04072794318199158, ssim: 0.04570603370666504\n",
      "423, L1: 0.07109180092811584, ssim: 0.06666749715805054\n",
      "424, L1: 0.0733092650771141, ssim: 0.08349400758743286\n",
      "425, L1: 0.0822066068649292, ssim: 0.0881226658821106\n",
      "426, L1: 0.17587202787399292, ssim: 0.2430676817893982\n",
      "427, L1: 0.05033344030380249, ssim: 0.04778015613555908\n",
      "428, L1: 0.054029833525419235, ssim: 0.06913012266159058\n",
      "429, L1: 0.08343128114938736, ssim: 0.09239643812179565\n",
      "430, L1: 0.06100674346089363, ssim: 0.05649751424789429\n",
      "431, L1: 0.08579210937023163, ssim: 0.10124492645263672\n",
      "432, L1: 0.1382099688053131, ssim: 0.17529475688934326\n",
      "433, L1: 0.21239972114562988, ssim: 0.26901644468307495\n",
      "434, L1: 0.06704381108283997, ssim: 0.1258183717727661\n",
      "435, L1: 0.08753770589828491, ssim: 0.10817992687225342\n",
      "436, L1: 0.07924285531044006, ssim: 0.08583372831344604\n",
      "437, L1: 0.08269406110048294, ssim: 0.21679162979125977\n",
      "438, L1: 0.059649527072906494, ssim: 0.06901490688323975\n",
      "439, L1: 0.10748173296451569, ssim: 0.2108595371246338\n",
      "440, L1: 0.1342848241329193, ssim: 0.15510433912277222\n",
      "441, L1: 0.06840657442808151, ssim: 0.08009451627731323\n",
      "442, L1: 0.09412024170160294, ssim: 0.11647439002990723\n",
      "443, L1: 0.05810222402215004, ssim: 0.06361716985702515\n",
      "444, L1: 0.09269958734512329, ssim: 0.09400177001953125\n",
      "445, L1: 0.09392348676919937, ssim: 0.07839655876159668\n",
      "446, L1: 0.15384723246097565, ssim: 0.19296795129776\n",
      "447, L1: 0.048115409910678864, ssim: 0.03767669200897217\n",
      "448, L1: 0.06383567303419113, ssim: 0.06605678796768188\n",
      "449, L1: 0.09487182646989822, ssim: 0.099892258644104\n",
      "450, L1: 0.16707096993923187, ssim: 0.17319250106811523\n",
      "451, L1: 0.14442120492458344, ssim: 0.26129859685897827\n",
      "452, L1: 0.11849634349346161, ssim: 0.1555403470993042\n",
      "453, L1: 0.09029483795166016, ssim: 0.1749444603919983\n",
      "454, L1: 0.05242181196808815, ssim: 0.04992032051086426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455, L1: 0.07434134930372238, ssim: 0.0922250747680664\n",
      "456, L1: 0.09858784079551697, ssim: 0.18295514583587646\n",
      "457, L1: 0.17381393909454346, ssim: 0.2208053469657898\n",
      "458, L1: 0.13513469696044922, ssim: 0.15482217073440552\n",
      "459, L1: 0.08193998038768768, ssim: 0.09691119194030762\n",
      "460, L1: 0.10298686474561691, ssim: 0.1109459400177002\n",
      "461, L1: 0.07968256622552872, ssim: 0.06657242774963379\n",
      "462, L1: 0.059610363095998764, ssim: 0.059905290603637695\n",
      "463, L1: 0.06045956164598465, ssim: 0.06798720359802246\n",
      "464, L1: 0.19083385169506073, ssim: 0.24148380756378174\n",
      "465, L1: 0.045080821961164474, ssim: 0.03660166263580322\n",
      "466, L1: 0.11044272035360336, ssim: 0.12912142276763916\n",
      "467, L1: 0.1096285954117775, ssim: 0.11498904228210449\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-f7d01114e235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#evaluate accuracy using L1 loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_L1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_L1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-4d1f8ff258b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleakyrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evaluate accuracy using L1 loss\n",
    "for i, data in enumerate(dataloader):\n",
    "    output = encdec(data['c1'])\n",
    "    truth = data['c2']\n",
    "    loss_L1 = criterion_L1(output, truth)\n",
    "    \n",
    "    output_norm = (output + 1) / 2\n",
    "    truth_norm = (truth+1) / 2\n",
    "    loss_ssim = 1 - criterion_ssim(output_norm, truth_norm)\n",
    "    \n",
    "    print(f'{i}, L1: {loss_L1}, ssim: {loss_ssim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
